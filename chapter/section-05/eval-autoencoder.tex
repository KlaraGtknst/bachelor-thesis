\subsection*{\ac{ae}}\label{subsec:evaluation-ae}

% architecture
In order to determine which architecture for the hidden or so-called latent space of the \ac{ae} is the best option, 
different architectures are tested and compared in terms of \ac{rsme} and cosine similarity.
The \ac{rsme} is calculated as given in \lst{lst:impl-rsme}.
The cosine similarity is calculated as given in \lst{lst:impl-cos_sim}.
Due to the fact that cosine similarity values are bound by 0 and 1, they are easier to rank than metrics that can yield any real number.
However, cosine similarity is usually applied to calculate the angle between two vectors and thus, one has to be cautious when interpreting the results.
For instance, the vectors $\left( 0, 1 \right)^T$ and $\left( 0, 2 \right)^T$ have a cosine similarity of 1, even though they are not the same vectors.
Since an \ac{ae} is supposed to reconstruct the input rather than return a dependent or related vector, this metric should be combined with a traditional metric.
The dataset used for the evaluation is a selection of 195 documents from the Bahamas dataset.

% RSME
\begin{listing}[htp]
    \begin{minted}{python3}
        rsme = np.linalg.norm(inverse_embedding - embeddings) 
                / np.sqrt(embeddings.shape[0])
    \end{minted}
    \caption[Computation of the \acs*{rsme}]{
        Computation of the \acs*{rsme} between the original and the reconstructed embedding.
    }
    \label{lst:impl-rsme}
\end{listing}

% cosine similarity
\begin{listing}[htp]
    \begin{minted}{python3}
        cos_sim = statistics.mean([np.dot(inverse_emb, embedding)
                /(np.linalg.norm(inverse_emb)*np.linalg.norm(embedding)) 
                for inverse_emb, embedding in zip(inverse_embedding, embeddings)])
    \end{minted}
    \caption[Computation of the cosine similarity]{
        Computation of the cosine similarity between the original and the reconstructed embedding.
    }
    \label{lst:impl-cos_sim}
\end{listing}

\begin{figure}[!htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includegraphics[width=1\textwidth]{images/embeddings/autoencoder/ae_score_plot.pdf}
    \caption[Different \acs*{ae} architectures and their reconstruction error]{The effect of different \acs*{ae} architectures on the reconstruction error.
    The error is measured in terms of \acs*{rsme} (blue bars) and cosine similarity (yellow bars) between the original and the reconstructed image.
    The smallest \acs*{rsme} and the biggest cosine similarity belong to the architecture best suited to this task and are coloured green.
    }
    \label{fig:eval-ae-architecture}
\end{figure}

The scores of different architectures are shown in \fig{fig:eval-ae-architecture}.
The x-axis displays the number of neurons in each layer for the respective experiments.
The input space is 4096-dimensional since that is the dimensionality of \infersent{} embeddings.
The output of the encoder is 2048-dimensional which is the maximum dimensionality supported by \databaseName{} for dense vectors.
While most of the architectures produce similar results, one architecture stands out.
Combining 2500-, 3000- and 3500-dimensional layers in the hidden space produces the worst \ac{rsme} results.
The smallest \ac{rsme} and the biggest cosine similarity are achieved by adding a 3500-dimensional layer in the hidden space.
However, the results of the best architecture do not differ greatly from the others.