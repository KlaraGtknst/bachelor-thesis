\chapter{Evaluation}\label{ch:evaluation}

Since the dataset has no ground truth, the procedure used to pick the parameter values is not comparable to ground truth-based approaches.
Hence, the evaluation is informal and the methods applied have arisen from regular consultation with experts from the tax office.
Run times of different configurations are measured and compared.
Parameters are chosen with respect to model-specific procedures, such as reachability plots for \ac{optics}.
The models are compared to each other and their composition to the baseline topic analysis model \ac{t2v}.


 % Database
 \section{Database}\label{sec:eval-db}
There is a variety of parameter values to choose from when working with databases and embeddings.
These parameters include similarity metrics and the choice of query types.

 \input{chapter/section-05/eval-similarity-measurements.tex}

 \input{chapter/section-05/elasticsearch_db.tex}

 % Eigendocs
 \input{chapter/section-05/eval-eigendocs.tex}
 
 % Embeddings
\section{Embeddings}\label{sec:eval-embeddings}
As discussed in \autoref{subsec:impl-embeddings}, there is a range of possible parameter values to choose from when implementing embedding models.
The section below states which findings have led to the parameter values applied in this work.

\input{chapter/section-05/eval-tfidf.tex}

\input{chapter/section-05/eval-doc2vec.tex}

\input{chapter/section-05/eval-infersent.tex}

\input{chapter/section-05/eval-universal-sent-enc.tex}

\input{chapter/section-05/eval-autoencoder.tex}

% Clustering
\input{chapter/section-05/eval-optics.tex}

% Eval models and baseline
\input{chapter/section-05/comparison_models.tex}

\input{chapter/section-05/comparison_baseline.tex}