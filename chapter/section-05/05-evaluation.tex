\chapter{Experimental evaluation}\label{ch:evaluation}

Since the dataset has no ground truth the procedure used to decide which parameters to use is not comparable to common ground truth-based thesis.
Hence, the evaluation is informal and was conducted after regular consultation with experts from the tax office.

 % Database
 \section{Database}\label{sec:eval-db}
 \input{chapter/section-05/eval-similarity-measurements.tex}

 \input{chapter/section-05/elasticsearch_db.tex}

 % Eigendocs
 \input{chapter/section-05/eval-eigendocs.tex}
 
 % Embeddings
\section{Embeddings}\label{sec:eval-embeddings}
\input{chapter/section-05/eval-autoencoder.tex}

\input{chapter/section-05/eval-tfidf.tex}

\input{chapter/section-05/eval-doc2vec.tex}

\input{chapter/section-05/eval-infersent.tex}

\input{chapter/section-05/eval-universal-sent-enc.tex}

% Clustering
\input{chapter/section-05/eval-optics.tex}





\section{Comparison of models}\label{sec:evaluation-models}

% parameters
Similar to \citeauthor{glove2014}'s work, in this work, for many models used, any unspecified parameters are set to their default values, 
assuming that they are close to optimal
acknowledging that this simplification should be revised in a more thorough analysis.

% comparing models (qualitative)
difference query responses for different models?
any images which produce unusual results?
