\subsection*{\acs{tfidf}}\label{subsec:evaluation-tfidf}

The main obstacle to overcome is the high dimensionality of the \ac{tfidf} embeddings.
Hence, the goal of the parameter selection is to find a way to reduce the dimensionality of the vocabulary to the maximum vector dimensionality of \databaseName{}.
However, the quality of the embeddings should not decline too much.

% parameter selection
The choice of the preprocessor was investigated with regard to the goal of minimizing the vocabulary size.
Both the default and a custom preprocessor were tested on a data corpus of 2048 randomly selected documents with regard to the vocabulary (size).
While the default preprocessor had a vocabulary size of 5893, the custom preprocessor had a size of 5585.
The relative difference likely shrinks with a larger data set, 
since the trend is already visible for two different data corpus sizes in \autoref{tab:tfidf-preprocessor-comparison}.
The custom preprocessor was chosen because it had a smaller vocabulary size.
The differences between both vocabularies were compared and visualized in \autoref{fig:differences-vocabularies}.

% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[]
    \caption[Comparison of the default and the custom \ac{tfidf} preprocessor]
    {Comparison of vocabulary sizes resulting from the default and the custom \ac{tfidf} preprocessor on different data corpus sizes.}
    \begin{tabular}{|p{0.2\textwidth}|p{0.3\textwidth}|p{0.3\textwidth}|p{0.14\textwidth}|}
    \hline
    \rowcolor[HTML]{C0C0C0} 
    {\color[HTML]{000000} \textbf{document corpus size M}} & {\color[HTML]{000000} \textbf{custom preprocessor vocabulary size A}} & {\color[HTML]{000000} \textbf{default preprocessor vocabulary size B}} & {\color[HTML]{000000} \textbf{(B-A)/M}} \\ \hline
    195                                                    & 1521                                                                  & 1641                                                                   & 120/195 = 0,6153846154                  \\ \hline
    2048                                                   & 5585                                                                  & 5893                                                                   & 308/2048 = 0,150390625                  \\ \hline
    \end{tabular}
    \label{tab:tfidf-preprocessor-comparison}
\end{table}

\begin{figure}%
    \centering
    \subfloat[\centering The terms only present in the vocabulary produced by the default preprocessor.]{{\includegraphics[width=6.8cm]{images/embeddings/tfidf/Word_cloud_in_vocabular_which_is_in_default_but_not_custom.pdf} }}%
    \qquad
    \subfloat[\centering The terms only present in the vocabulary obtained from the custom preprocessor.]{{\includegraphics[width=6.8cm]{images/embeddings/tfidf/Word_cloud_in_vocabular_which_is_in_custom_but_not_default.pdf} }}%
    \caption[\wordcloud{}s for different \ac{tfidf} preprocessors]{The \wordcloud{}s visualize which words are unique to both vocabularies.}%
    \label{fig:differences-vocabularies}%
\end{figure}

% two fields in db
Initially, there should have been two different \ac{tfidf} models.
The first one should have been used to obtain documents which are similar to the query document.
Therefore, terms that occur only once in the corpus should have been removed from the vocabulary.
The second approach should have been used to obtain specific documents from the corpus.
Hence, the vocabulary should consist of very document-specific terms and thus, \texttt{max\_df} would have been relatively low, to omit terms that occur in many documents.
However, the restrictions imposed by the database implementation in terms of dimensionality limitations
made it impossible to explore many parameter ranges.
Therefore, only one \ac{tfidf} model was used in the end, whose parameters \texttt{min\_df} and \texttt{max\_df} were set to values which kept the vocabulary and thus,
the dimensionality of the embeddings reasonably small.