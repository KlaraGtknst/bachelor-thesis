
\section{Similarity Measurement}\label{sec:similarity-measurement}

% Embeddings represent texts as numerical vectors in a $N$-dimensional space.
% This space is called \ac{vsm} \cite{soft_cosine2014}.
Embeddings not only facilitate human interpretability of relationships between texts, 
%using the text's respective point in a $N$-dimensional space, 
but they also enable the use of metrics, i.e. similarity measures, to quantify the similarity between texts \cite{IR2011, euclidean_l2_norm2015}.

There are several similarity measures, such as the dot product quantifying the number of shared tokens of two texts, 
the (soft) cosine similarity \cite{soft_cosine2014, soft_cosine2017}, which is the normalized dot product and calculates the angle between two vectors, 
and many more \cite{IR2011, euclidean_l2_norm2015, HfsentTrans2019}.
The following section outlines a selection of similarity measures.


\subsection{Euclidian distance}\label{subsec:euclidian-distance}

The \textit{euclidian distance} is a distance measure.
In order to measure the distance between two points in a $N$-dimensional space, 
the root of the sum of squared distances between the respective values of every dimension is calculated.
The Euclidean (L2) norm between two points $a, b$ is defined as $d_E(a,b) = \sqrt{\sum_{i=1}^{N}(a_i - b_i)^2}$ c.f. \cite{euclidean_l2_norm2015}.


\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

% In the traditional bag-of-words approach the texts are represented as vectors of \ac{tfidf} coefficients \cite{soft_cosine2017}.
% Without further processing, the vector is of size $N$, $N$ being the number of different words of the texts \cite{soft_cosine2017}.

The similarity between two texts is measured by the cosine of the angle between their respective real-valued vectors.
The cosine similarity is defined in \autoref{eq:cosine-similarity} from \cite{soft_cosine2014}.
For positive vectors, for instance, produced by \ac{tfidf}, it is a value between $0$ and $1$.
% angle can not be greater than 90 degrees, bc vectors have positive values
%According to \citeauthor{soft_cosine2014}, the formula has a time and space complexity of $O(N)$ for a pair of $N$-dimensional vectors.

\begin{equation}
    cosine(a,b) = \frac{a \cdot b}{\left\| a \right\| \times \left\| b \right\|} = \frac{\sum_{i=1}^{N}a_{i}b_{i}}{\sqrt{\sum_{i=1}^{N}{a}^2_{i}}\sqrt{\sum_{i=1}^{N}{b}^2_{i}}}
    \label{eq:cosine-similarity}
\end{equation}

The formula from \autoref{eq:cosine-similarity} assumes that the vectors, which span the \ac{vsm} are orthogonal and thus, 
completely independent.
However, in practical applications, the index terms which span the \ac{vsm} are often semantically dependent.

%\input{chapter/section-03/soft_consine_similarity.tex}