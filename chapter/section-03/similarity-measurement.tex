
\section{Similarity Measurement}\label{sec:similarity-measurement}

Embeddings represent texts as numerical vectors in a $N$-dimensional space.
This space is called \ac{vsm} \cite{soft_cosine2014}.
These vectors not only facilitate human interpretability of relationships between texts, 
%using the text's respective point in a $N$-dimensional space, 
but they also enable the use of metrics, i.e. similarity measures, to quantify the similarity between texts \cite{IR2011, euclidean_l2_norm2015}.

There are several similarity measures, such as the dot product quantifying the number of shared tokens of two texts, 
the (soft) cosine similarity \cite{soft_cosine2014, soft_cosine2017}, which is the normalized dot product and calculates the angle between two vectors, 
and many more \cite{IR2011, euclidean_l2_norm2015, HfsentTrans2019}.
The following section outlines a few similarity measures.


\subsection{Euclidian distance}\label{subsec:euclidian-distance}

The \textit{euclidian distance} is a distance measure.
In order to measure the distance between two points in a $N$-dimensional space, 
the root of the sum of squared distances between the respective values of every dimension is calculated.
The distance function Euclidean (L2) norm is given in \autoref{eq:euclidian-distance} c.f. \cite{euclidean_l2_norm2015}.
The points $a, b$ correspond to objects $d_1, d_2$.

\begin{equation}
    d_E(a,b) = \sqrt{\sum_{i=1}^{N}(a_i - b_i)^2}
    \label{eq:euclidian-distance}
\end{equation}


\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

% In the traditional bag-of-words approach the texts are represented as vectors of \ac{tfidf} coefficients \cite{soft_cosine2017}.
% Without further processing, the vector is of size $N$, $N$ being the number of different words of the texts \cite{soft_cosine2017}.

The similarity between two texts is measured by the cosine of the angle between their respective real-valued vectors \cite{soft_cosine2014}.
The cosine similarity is defined in \autoref{eq:cosine-similarity} from \cite{soft_cosine2014}.
The dot-product is calculated using $a \cdot b = \sum_{i=1}^{N}a_{i}b_{i}$.
It is normalized with $\left\| x \right\| = \sqrt{x \cdot x}$ to unit Euclidean length.
The cosine similarity is a value between $0$ and $1$ for positive vectors.
% angle can not be greater than 90 degrees, bc vectors have positive values
According to \citeauthor{soft_cosine2014}, the formula has a time and space complexity of $O(N)$ for a pair of $N$-dimensional vectors.

\begin{equation}
    cosine(a,b) = \frac{a \cdot b}{\left\| a \right\| \times \left\| b \right\|} = \frac{\sum_{i=1}^{N}a_{i}b_{i}}{\sqrt{\sum_{i=1}^{N}{a}^2_{i}}\sqrt{\sum_{i=1}^{N}{b}^2_{i}}}
    \label{eq:cosine-similarity}
\end{equation}

The formula from \autoref{eq:cosine-similarity} assumes that the vectors, which span the \ac{vsm} are orthogonal and thus, 
completely independent.
However, in practical applications, this often is not the case.


\subsection{Soft Cosine Similarity}\label{subsec:soft-cosine-similarity}

This similarity measure not only evaluates whether two texts consist of the same words but 
also takes into account the semantic (word-level) similarity or lexical relation of different words of the texts \cite{soft_cosine2017}.
Hence, it improves the shortcomings of the traditional cosine similarity measure, 
which assumes the tokens of the vocabulary are completely independent of each other \cite{soft_cosine2014}.

According to \citeauthor{soft_cosine2014}, in order to model this additional information, more dimensions are added to the \ac{vsm}.
These dimensions can be obtained, for instance, by multiplying the mean of two features of one vector with the similarity between them \cite{soft_cosine2014}.
The similarity can be calculated by using Levenshtein distance, i.e. the number of operations necessary to convert one string into another, 
or using a dictionary of synonyms.

Since this approach no longer assumes that different words are independent of each other, 
the basis vectors which span the \ac{vsm} are no longer presumed orthogonal.
The formula for the soft cosine similarity is defined in \autoref{eq:soft-cosine-similarity} from \cite{soft_cosine2014}.
The similarity $s_{ij}$ between the $i$-th and $j$-th basis vector is obtained using a similarity measure, such as synonymy.

\begin{equation}
    soft\_cosine(a,b) = \frac{\sum_{i=1}^{N}\sum_{j=1}^{N}s_{ij}a_{i}b_{j}}{\sqrt{\sum_{i=1}^{N}\sum_{j=1}^{N}s_{ij}a_{i}a_{j}}\sqrt{\sum_{i=1}^{N}\sum_{j=1}^{N}s_{ij}b_{i}b_{j}}}
    \label{eq:soft-cosine-similarity}
\end{equation}

According to \citeauthor{soft_cosine2017}, the similarity between two texts is non-zero as soon as they share related words \cite{soft_cosine2017}.
If there is no similarity between different features, 
the soft cosine similarity from \autoref{eq:soft-cosine-similarity} is equal to the cosine similarity from \autoref{eq:cosine-similarity}.
The time and space complexity of the soft cosine similarity is $O(N^2)$ \cite{soft_cosine2014}.
In order to reduce the complexity, \citeauthor{soft_cosine2014} propose the usage of a sparse similarity matrix which only stores $s_{ij} > t$, 
$t$ being a threshold.