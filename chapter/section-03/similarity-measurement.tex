\section{Similarity measurement}\label{sec:similarity-measurement}

Embeddings not only facilitate human interpretability of relationships between texts, 
but they also enable the use of metrics, i.e.\ similarity measures, to quantify the similarity between texts \cite{IR2011, euclidean_l2_norm2015}.

There are several similarity measures, such as the dot product quantifying the number of shared tokens of two texts, 
the (soft) cosine similarity \cite{soft_cosine2014, soft_cosine2017}, which is the normalized dot product and calculates the angle between two vectors, 
and many more \cite{IR2011, euclidean_l2_norm2015, HfsentTrans2019}.
The following section outlines a selection of similarity measures.


\subsection{Euclidian distance}\label{subsec:euclidian-distance}

The \textit{euclidian distance} is a distance measure.
In order to measure the distance between two vectors in a $N$-dimensional space, 
the root of the sum of squared distances between the respective values of every dimension is calculated.
The Euclidean (L2) norm between two vectors $a, b$ is defined in \autoref{eq:euclidean-distance} \cite{euclidean_l2_norm2015}.
The distance is zero if the vectors are identical, i.e. $a = b$.
The more $a$ and $b$ differ, the greater is the distance $d_E(a,b)$ between them.

\begin{equation}
    d_E(a,b) = \sqrt{\sum_{i=1}^{N}(a_i - b_i)^2}
    \label{eq:euclidean-distance}
\end{equation}


\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

\begin{figure}[!htb]%
    \centering
    \subfloat[\centering Similar vectors $a, b$.]
    {{\includesvg[width=5cm]{images/cosine_similarity/similar_vectors_cos_sim.svg}}}%
    \qquad
    \subfloat[\centering Dissimilar vectors $a, b$.]
    {{\includesvg[width=5cm]{images/cosine_similarity/dissimilar_vectors_cos_sim.svg} }}%
    \caption[Cosine Similarity]{Cosine Similarity between two vectors considers the angle between them.}%
    \label{fig:cos_sim_vectors}%
\end{figure}

% In the traditional bag-of-words approach the texts are represented as vectors of \ac{tfidf} coefficients \cite{soft_cosine2017}.
% Without further processing, the vector is of size $N$, $N$ being the number of different words of the texts \cite{soft_cosine2017}.

The similarity between two texts is measured by the cosine of the angle between their respective real-valued vectors.
The cosine similarity is defined in \autoref{eq:cosine-similarity} \cite{soft_cosine2014}.
For positive vectors, for instance, produced by \ac{tfidf}, it is a value between $0$ and $1$.
If the angle is close to zero degrees, the cosine similarity is close to $1$ and the vectors are similar.
If the angle is close to $90$ degrees, the cosine similarity is close to $0$ and the vectors are dissimilar.
Both a similar and a dissimilar pair of vectors are depicted in \autoref{fig:cos_sim_vectors}.

\begin{equation}
    cosine(a,b) = \frac{a \cdot b}{\left\| a \right\| \times \left\| b \right\|} = \frac{\sum_{i=1}^{N}a_{i}b_{i}}{\sqrt{\sum_{i=1}^{N}{a}^2_{i}}\sqrt{\sum_{i=1}^{N}{b}^2_{i}}}
    \label{eq:cosine-similarity}
\end{equation}

The formula from \autoref{eq:cosine-similarity} assumes that the vectors, which span the \ac{vsm} are orthogonal and thus, 
completely independent.
However, in practical applications, the index terms which span the \ac{vsm} are often semantically dependent.

%\input{chapter/section-03/soft_consine_similarity.tex}