\subsection{\acl*{nn}}\label{sec:neural_network}

% architecture
A \ac{nn} is a \ac{ml} model which consists of multiple layers of nodes.
A node, or so-called neuron, takes an input vector and produces an output vector.
The output is derived from the calculation of a weighted sum of the inputs and an activation function \cite{KI2022}.
The architecture of a \ac{nn} is shown in \autoref{fig:architecture_nn}.
The first and last layers are called input and output layers, respectively.
The layers between the input and output layers are called hidden layers.
If a \ac{nn} has more than one hidden layer, it is called a \ac{dnn} and working with \acp{dnn} is considered deep learning.
To propagate the input through the network the layers are connected.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.5\textwidth]{images/embeddings/neural_network/Neural_network.svg}
    \caption[Architecture of a \acs*{nn}]
    {Architecture of a \acs*{nn}. 
    The input layer is the first layer of the network.
    It receives the input data \textbf{x}.
    The output layer is the last layer of the network and returns \textbf{y}.
    Between the input and output layers, there are one or more hidden layers.
    }
    \label{fig:architecture_nn}
\end{figure}

% training
\acp{nn} are trained using the backpropagation algorithm which reduces the error between the predicted and the actual output iteratively.
While data is propagated in a forward direction through the network, the error is propagated in a backward direction.
The weights of the layers are adjusted according to the error \cite{KI2022}.
