\subsection{\acl*{tfidf}}\label{subsec:tfidf}

\ac{tfidf} provides a numerical representation of a word in a document \cite{clusteringDocs2020}.
It considers the frequency $f_{w_{j}, d}$  of a word $w_{j}$ in a document $d$ and the frequency of a word in the whole corpus. 
The frequency $f_{w_{j}, d}$ is defined as $\frac{\# \text{occurrences of } w_{j} \text{ in }d}{\# \text{words in }d}$.

\ac{tfidf} is calculated using $TFIDF(w_{j}, d, D) = TF(w_{j}, d) \cdot IDF(w_{j}, D)$ from \cite{clusteringDocs2020}.
Each entry of a \ac{tfidf} embedding vector represents the \ac{tfidf} value of a word in a document.
The \ac{tf} is determined utilizing $TF(w_{j}, d) = f_{w_{j}, d}$, whereas the \ac{idf} is computed by $IDF(w_{j}, D) = \log_2\frac{M}{M_{j}}$, 
$M_{j}$ being the number of documents the term $w_{j}$ appears in.

\ac{idf} measures the importance of a term $w_{j}$ in the corpus of documents $D$
under the assumption that a term's importance to the data corpus is inversely proportional to its occurrence frequency \cite{tfidf2008}.
In other words: Terms which appear in many documents are not as important and thus, weighted less than document-specific terms. 
The calculation of \ac{tf} and \ac{idf} is visualized exemplary in \autoref{fig:tfidf-calculation}.


\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.7\textwidth]{images/embeddings/tfidf/tfidf.svg}
    \caption[Exemplary calculation of \ac{tf} and \ac{idf} values]{
        Exemplary calculation of \ac{tf} and \ac{idf} for a document corpus $D$: 
        \ac{tf} only considers the documents of interest while 
        \ac{idf} incorporates the importance of the word with respect to $D$.
    }
    \label{fig:tfidf-calculation}
\end{figure}

%According to \citeauthor{tfidf2008}, the computation complexity of \ac{tfidf} embeddings is $O(V \cdot M)$.
\ac{tfidf} has several drawbacks \cite{clusteringDocs2020,tfidf2008}:
\begin{itemize}
    \item \ac{tfidf} does not consider semantic similarities between words.
    \item \ac{tfidf} does not take into account the order of words in a document.
    \item \ac{tfidf} often produces high dimensional representations which have to be postprocessed to reduce their dimensionality, e.g., by using \ac{pca}.
    %\item The embeddings are not derived from a mathematical model of term distribution and thus, are occasionally criticised as not well reasoned.
\end{itemize}

% TODO: advantages of tfidf?