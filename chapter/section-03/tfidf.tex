\subsection{\acl*{tfidf}}\label{subsec:tfidf}

\acf{tfidf} provides a numerical representation of a word in a document.
Let a corpus of documents be denoted $D= \left\{d_1, d_2, ..., d_M  \right\}$, $M$ being the total number of documents in the corpus. 
Let a sequence of terms $w_{j} \in V$ be denoted a document $d_i = \left\{w_{1}, w_{2}, ...\right\}$, 
$V$ being the vocabulary, 
i.e.\ set of distinct words \cite{clusteringDocs2020}.

The \ac{tfidf} model considers the frequency $f_{w_{j}, d}$  of a word $w_{j}$ in a document $d$ and the frequency of a word in the whole corpus. 
The frequency $f_{w_{j}, d}$ is defined in \autoref{eq:tfidf_frequency}, $w'_j$ being the number of occurrences of $w_j$ in $d$.

\begin{align}
    f_{w_{j}, d} &= \frac{w'_{j}}{\sum_{k \in d} w'_k}\label{eq:tfidf_frequency}\\
    TFIDF(w_{j}, d, D) &= TF(w_{j}, d) \cdot IDF(w_{j}, D)\label{eq:tfidf_calculation}\\
    TF(w_{j}, d) &= f_{w_{j}, d}\label{eq:tf_calculation}\\
    IDF(w_{j}, D) &= \log_2\frac{M}{M_{j}}\label{eq:idf_calculation}
\end{align}

\ac{tfidf} is calculated using \autoref{eq:tfidf_calculation} from \cite{clusteringDocs2020}.
Each entry of a \ac{tfidf} embedding vector represents the \ac{tfidf} value of a word in a document.
Hence, the embedding vector is of the same length as the vocabulary of the corpus.
The \ac{tf} is determined utilizing \autoref{eq:tf_calculation}, 
whereas the \ac{idf} is computed by \autoref{eq:idf_calculation}, 
$M_{j}$ being the number of documents the term $w_{j}$ appears in.

\ac{idf} measures the importance of a term $w_{j}$ in the corpus of documents $D$
under the assumption that a term's importance to the data corpus is inversely proportional to its occurrence frequency \cite{tfidf2008}.
In other words: Terms which appear in many documents are not as important and thus, weighted less than document-specific terms. 
The calculation of \ac{tf} and \ac{idf} is visualized exemplary in \autoref{fig:tfidf-calculation}.


\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.7\textwidth]{images/embeddings/tfidf/tfidf.svg}
    \caption[Exemplary calculation of \acs*{tf} and \acs*{idf} values]{
        Exemplary calculation of \acs*{tf} and \acs*{idf} for a document corpus $D$: 
        \acs*{tf} only considers the documents of interest while 
        \acs*{idf} incorporates the importance of the word with respect to $D$.
    }
    \label{fig:tfidf-calculation}
\end{figure}

%According to \citeauthor{tfidf2008}, the computation complexity of \ac{tfidf} embeddings is $O(V \cdot M)$.
\ac{tfidf} has several drawbacks \cite{clusteringDocs2020,tfidf2008}:
\begin{itemize}
    \item \ac{tfidf} does not consider semantic similarities between words.
    \item \ac{tfidf} does not take into account the order of words in a document.
    \item \ac{tfidf} often produces high dimensional representations which have to be postprocessed to reduce their dimensionality, e.g., by using \ac{pca}.
    %\item The embeddings are not derived from a mathematical model of term distribution and thus, are occasionally criticised as not well reasoned.
\end{itemize}

% TODO: advantages of tfidf?