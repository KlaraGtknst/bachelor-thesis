\section{\acl{ae}}\label{subsec:impl-autoencoder}

In this work, the \ac{ae} is used to reduce the dimensionality of the \infersent{} embedding.
Since the \infersent{} model is pretrained, it is not possible to change the dimensionality of the embedding without a considerably big effort,
i.e. retraining the model on a sufficiently large data corpus and reconfiguring the model's parameters.
Moreover, retraining the model would destroy the purpose of its presence in this work, which is to provide a pretrained model and thus, 
reducing the complexity of training an own model.
Therefore, it is not feasible to change the dimensionality of the \infersent{} embedding, but rather adding a supplementary layer after the model 
produce the final embedding.
Hence, the idea is to use the encoder of an \ac{ae} to reduce the dimensionality of the \infersent{} embedding.

The architecture and implementation from \lst{lst:impl_ae} was provided from 
\href{https://blog.paperspace.com/autoencoder-image-compression-keras/}{a blog post using keras}.
\autoref{fig:impl-encoder} \autoref{fig:impl-ae}

\begin{figure}[h] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.6\textwidth]{images/compression/autoencoder/encoder-impl.svg}
    \caption{Architecture of the encoder of the \ac{ae}}
    \label{fig:impl-encoder}
\end{figure}

\begin{figure}[h] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=1.0\textwidth]{images/compression/autoencoder/autoencoder-impl.svg}
    \caption{Architecture of the \ac{ae}}
    \label{fig:impl-ae}
\end{figure}

\begin{listing}[htp]
    \begin{minted}{python3}
    # Encoder
    x = layers.Input(shape=(input_shape), name="encoder_input")
    encoder_dense_layer1 = layers.Dense(units=300, name="encoder_dense_1")(x)
    encoder_activ_layer1 = layers.LeakyReLU(name="encoder_leakyrelu_1")(encoder_dense_layer1)
    encoder_dense_layer2 = layers.Dense(units=latent_dim, name="encoder_dense_2")(encoder_activ_layer1)
    encoder_output = layers.LeakyReLU(name="encoder_output")(encoder_dense_layer2)
    encoder = models.Model(x, encoder_output, name="encoder_model")

    # Decoder
    decoder_input = layers.Input(shape=(latent_dim), name="decoder_input")
    decoder_dense_layer1 = layers.Dense(units=300, name="decoder_dense_1")(decoder_input)
    decoder_activ_layer1 = layers.LeakyReLU(name="decoder_leakyrelu_1")(decoder_dense_layer1)
    decoder_dense_layer2 = layers.Dense(units=input_shape, name="decoder_dense_2")(decoder_activ_layer1)
    decoder_output = layers.LeakyReLU(name="decoder_output")(decoder_dense_layer2)
    decoder = models.Model(decoder_input, decoder_output, name="decoder_model")

    # Autoencoder
    ae_input = layers.Input(shape=(input_shape), name="AE_input")
    ae_encoder_output = encoder(ae_input)
    ae_decoder_output = decoder(ae_encoder_output)
    ae = models.Model(ae_input, ae_decoder_output, name="AE")
    ae.compile(loss="mse", optimizer=optimizers.legacy.Adam(learning_rate=0.0005))
    ae.fit(x_train, x_train, epochs=20, batch_size=256, shuffle=True, validation_data=(x_test, x_test))
    \end{minted}
    \caption{Architecture of \ac{ae} using \texttt{tensorflow.keras}. 
    }
    \label{lst:impl_ae}
\end{listing}