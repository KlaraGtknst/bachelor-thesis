\section{Embeddings}\label{sec:embeddings}

Usually, \ac{ml} techniques require textual inputs to be converted to embeddings \cite{SentRep2014}.
Embeddings are numerical representations of words, sentences or texts.
They can be used to present the textual data as real-valued vectors in a \ac{vsm}.
A simple example of a \ac{vsm} in the \ac{nlp} context is shown in \autoref{fig:vsm_example}.
A \ac{vsm} is a $N$-dimensional space \cite{soft_cosine2014}.
\acp{vsm} are commonly used due to their conceptual simplicity and because spatial proximity correlates with semantic proximity 
\cite{tfidf2008, UniversalSentEnc2018, HfsentTrans2019, Top2Vec2020}.
Representations in a \ac{vsm} can improve the performance in \ac{nlp} tasks \cite{SkipGram2013}.
The following section outlines the fundamentals of a selection of embeddings.

\begin{figure}[!htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.4\textwidth]{images/embeddings/VSM/VSM_example.svg}
    \caption[A simple \acs*{vsm}]
    {A simple \acs*{vsm}. 
    The words are represented as vectors in a two-dimensional space.
    Since \textit{wine} is semantically more similar to \textit{drink} than to \textit{food}, the vectors are closer together.
    }
    \label{fig:vsm_example}
\end{figure}


\input{chapter/section-03/neural_networks.tex}

\input{chapter/section-03/tfidf.tex}

\input{chapter/section-03/doc2vec.tex}

\input{chapter/section-03/univ_sent_encoder.tex}

\input{chapter/section-03/infersent.tex}

\input{chapter/section-03/sent_transformers_hf.tex}