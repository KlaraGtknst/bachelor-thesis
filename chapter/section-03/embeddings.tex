\section{Embeddings}\label{sec:embeddings}

Usually, \ac{ml} techniques require textual inputs to be converted to embeddings \cite{SentRep2014}.
Embeddings are numerical representations of words, sentences or texts.
They can be used to present the textual data as real-valued vectors in a \ac{vsm}.
A \ac{vsm} is a $N$-dimensional space \cite{soft_cosine2014}.
Each dimension of a \ac{vsm} corresponds to an index term, which is dependent on the embedding model.
Every document embedding dimension explains the importance of the corresponding index term to the document.
\acp{vsm} are commonly used due to their conceptual simplicity and because spatial proximity serves as a metaphor for semantic proximity 
\cite{tfidf2008, UniversalSentEnc2018, HfsentTrans2019, Top2Vec2020}.
Representations in a \ac{vsm} can improve the performance in \ac{nlp} tasks \cite{SkipGram2013}.
According to \citeauthor{tfidf2008}, when representing text the first step is indexing, i. e. assigning indexing terms to the document.
The second task is to assign weights to the terms that correspond to the importance of the term in the document.
The weights assigned depend on the model.

The following section outlines the fundamentals of a selection of embeddings.
Let a corpus of documents be denoted $D= \left\{d_1, d_2, ..., d_M  \right\}$, 
the number of documents in the dataset $M = \left\| D \right\|$,
and a sequence of terms $w_{ij}$ or so-called document $d_i = \left\{w_{i1}, w_{i2}, ..., w_{iV}  \right\}$, 
$V$ being the length of the vocabulary, 
i. e. set of distinct words \cite{clusteringDocs2020}, and $j \in [0, V]$.


\input{chapter/section-03/tfidf.tex}

\input{chapter/section-03/doc2vec.tex}

\input{chapter/section-03/univ_sent_encoder.tex}

\input{chapter/section-03/infersent.tex}

\input{chapter/section-03/sent_transformers_hf.tex}