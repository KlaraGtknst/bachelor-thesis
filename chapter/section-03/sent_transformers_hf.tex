\subsection{\acl*{sbert}}\label{subsec:hf-sent-ransformers}

\ac{sbert} is an enhancement of \ac{bert}.
% BERT
\ac{bert} is a pre-trained transformer network.
It predicts a target value, for i. e. sentence classification or sentence-pair regression tasks, based on two input sentences \cite{HfsentTrans2019}.
The input sentences are separated by a special token \texttt{[SEP]}.
The base model applies multi-head attention over 12 transformer layers, whereas the large model applies multi-head attention over 24 transformer layers.
The final label is derived from a regression function, which receives the output of the $12^\text{th}$ or $24^\text{th}$ layer, respectively.
\citeauthor{HfsentTrans2019} state that \ac{bert} is not suitable for specific pair regression tasks, 
since the number of input sentence combinations is too big.
Another shortcoming of \ac{bert} is that it does not produce independent embeddings for single sentences.
Moreover, \citeauthor{HfsentTrans2019} found that common similarity measurements, for instance, the ones discussed in \autoref{sec:similarity-measurement}, 
do not perform well on sentence embeddings produced by \ac{bert} \cite{HfsentTrans2019}.

% SBERT
\ac{sbert} provides fixed-sized embeddings for single sentences \cite{HfsentTrans2019}.
It differs from \ac{bert} in terms of architecture, since it adds a pooling layer after the \ac{bert} model.
\citeauthor{HfsentTrans2019} compare different pooling strategies, such as using the output of the \texttt{CLS} (i. e. first) token, mean pooling and max pooling.
The architecture of a single \ac{sbert} network is depicted in \autoref{fig:sbert}.
In order to work with multiple input sentences at the same time, siamese and triplet network architectures, 
i. e. multiple \ac{bert} networks with tied weights, are constructed.
To perform classification or inference tasks layers are added on top of the \ac{sbert} network.
% training corpus
\ac{sbert} is trained on the \ac{snli} dataset.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.7\textwidth]{images/embeddings/SBERT/SBERT.svg}
    \caption[Architecture of \ac{sbert}]{Architecture of \ac{sbert} cf. \cite{HfsentTrans2019}.
    \ac{bert} is extended by a pooling layer.
    The input is a sentence and the output is a fixed-sized embedding.
    }
    \label{fig:sbert}
\end{figure}

% performance
According to \citeauthor{HfsentTrans2019}, \ac{sbert} outperforms \infersent{} and \ac{use} on Semantic Textual Similarity tasks 
and on SentEval, which is an evaluation toolkit for sentence embeddings \cite{HfsentTrans2019}.
%Moreover, due to \ac{sbert}'s transformer architecture, it is more computationally efficient than \ac{use} on \acp{gpu}.