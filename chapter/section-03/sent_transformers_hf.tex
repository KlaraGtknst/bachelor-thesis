\subsection{\acl*{sbert}}\label{subsec:hf-sent-ransformers}

\acf{sbert} is an enhancement of \ac{bert}.
The applicability of \ac{bert} is limited because it does not produce independent embeddings for single sentences \cite{HfsentTrans2019}.
Moreover, \citeauthor{HfsentTrans2019} found that common similarity measurements, for instance, the ones discussed in \autoref{sec:similarity-measurement}, 
do not perform well on sentence embeddings produced by \ac{bert}.

% BERT
\ac{bert} is a pre-trained transformer network which 
predicts a target value based on two input sentences for sentence classification or sentence-pair regression tasks \cite{HfsentTrans2019}.
The \ac{bert} base model applies multi-head attention over 12 transformer layers, whereas the large model applies multi-head attention over 24 transformer layers.
The attention mechanism enables access to all hidden states as opposed to only the last hidden state \cite{attention_book2023}.
It derives its output vector as a dynamic weighted sum of the hidden states.
The final label is derived from a regression function, which receives the output of the $12^\text{th}$ or $24^\text{th}$ layer, respectively.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.7\textwidth]{images/embeddings/SBERT/SBERT.svg}
    \caption[Architecture of \acs*{sbert}]{Architecture of \acs*{sbert} cf. \cite{HfsentTrans2019}.
    \acs*{bert} is extended by a pooling layer.
    The input is a sentence and the output is a fixed-sized embedding.
    }
    \label{fig:sbert}
\end{figure}

% SBERT
\ac{sbert} provides fixed-sized embeddings for single sentences \cite{HfsentTrans2019}.
It differs from \ac{bert} in terms of architecture, since it adds a pooling layer after the \ac{bert} model.
\citeauthor{HfsentTrans2019} compare different pooling strategies, such as using the output of the \texttt{CLS} (i.e.\ first) token, mean pooling and max pooling.
The architecture of a single \ac{sbert} network is depicted in \autoref{fig:sbert}.
In order to work with multiple input sentences at the same time, siamese and triplet network architectures, 
i.e.\ multiple \ac{bert} networks with tied weights, are constructed.
To perform classification or inference tasks, layers are added on top of the \ac{sbert} network.
% training corpus
\ac{sbert} is trained on the \ac{snli} dataset \cite{HfsentTrans2019, snli_dataset}.

% performance
According to \citeauthor{HfsentTrans2019}, \ac{sbert} outperforms \infersent{} and \ac{use} on Semantic Textual Similarity tasks 
and on SentEval, which is an evaluation toolkit for sentence embeddings \cite{HfsentTrans2019}.
%Moreover, due to \ac{sbert}'s transformer architecture, it is more computationally efficient than \ac{use} on \acp{gpu}.