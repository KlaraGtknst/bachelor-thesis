\subsection{\acl*{d2v}}\label{subsec:doc2vec}

Another term used for \ac{d2v} is \textit{Paragraph Vector} \cite{clusteringDocs2020, SentRep2014}.
\ac{d2v} addresses \ac{tfidf} drawbacks by encoding texts as $N-$dimensional vectors learnt using the words' context \cite{clusteringDocs2020}.
$N \in \mathbb{N}$ can be chosen arbitrarily.
It preserves semantic similarities between words and encodes linguistic regularities and patterns \cite{SkipGram2013}.
The model handles inputs of different lengths, i.e.\ inputs can be sentences, paragraphs or documents.

% semantic similarity = similar embeddings
\ac{d2v} is an adaption of the \ac{w2v} model, which maps words into a \ac{vsm} \cite{clusteringDocs2020}.
Both approaches assume that words appearing in similar contexts are semantically similar. 
Hence, words which often appear in the same context produce similar embeddings.

% obtain embeddings
The \ac{d2v} embedding is obtained using a shallow \ac{nn}, i.e.\ the \ac{nn} has only one hidden layer.
The embeddings are created by the hidden layer.
There are two \ac{d2v} approaches to designing the architecture of the \ac{nn}:
\begin{itemize}
    \item \ac{pvdm}: 
        Predicts a word given a context \cite{SentRep2014, WordRep2013}.
    \item \ac{pvdbow}: 
        Predicts the context given a word \cite{EmbDist2015, SkipGram2013, SentRep2014}.
\end{itemize}

\begin{figure}%
    \centering
    \subfloat[\centering \acs*{cbow} architecture cf. \cite{WordRep2013}.]
    {{\includesvg[width=7cm]{images/embeddings/doc2vec/CBOW.svg}}}%
    \qquad
    \subfloat[\centering \acs*{pvdm} architecture cf. \cite{SentRep2014}.]
    {{\includesvg[width=6cm]{images/embeddings/doc2vec/PV-DM.svg} }}%
    \caption[\acs*{cbow} and \acs*{pvdm} architecture]{Both approaches predict the centre word \textbf{w(t)} using the context.
    \acs*{pvdm} is an adaption of \acs*{cbow} to work on a set of documents or paragraphs instead of words.
    }%
    \label{fig:pvdm}%
\end{figure}
 
% context to centre word (PVDM)
The \ac{pvdm} algorithm considers words within a sliding window and their document the context of a centre word \cite{SentRep2014}.
The document vector is added to incorporate the document's topic and thus, acts like a memory \cite{SentRep2014, Top2Vec2020}.
% word vectors
It encodes the context words into vectors via the \ac{w2v} \ac{cbow} model \cite{glove2014}.
% document vector
Each document is mapped to a vector using an additional document-to-vector matrix.
% result
The vectors can be concatenated, averaged or summed up \cite{SentRep2014}.
The resulting vector is the prediction of the central word.
The \ac{cbow} and the \ac{pvdm} approach are displayed in \autoref{fig:pvdm}.