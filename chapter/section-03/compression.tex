\section{Compression of data}\label{sec:compression}

According to \citeauthor{clusteringDocs2020}, a decomposition of data that preserves the inner structure in inherent clusters. 
When data analysis techniques are applied to reasonably low-dimensional data, the results usually improve.
Moreover, compressed data is less memory-consuming and often less difficult to interpret by humans 
since there are more methods to visualize low-dimensional data.
In the following, two approaches to reduce the dimensionality of data are presented.

\subsection{\acl*{ae}}\label{subsec:autoencoder}

The idea of this approach is to find a meaningful low-dimensional version of the input.
The high-dimensional data is encoded into a low-dimensional representation using the encoder of an undercomplete \ac{ae} \cite{autoencoder2020}.
Hence, the output of the latent space corresponds to the input's embedding. 
The low-dimensional representation can be decoded into an approximation of the high-dimensional original using the decoder of the \ac{ae}.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.4\textwidth]{images/embeddings/autoencoder/AE.svg}
    \caption[Structure of an \ac{ae}]
    {Structure of an \acs*{ae} cf. \cite{autoencoder2020}.
    The six-dimensional input is encoded into a three-dimensional representation.
    This encoding is decoded into a six-dimensional approximation of the original input.}
    \label{fig:ae}
\end{figure}

An undercomplete \ac{ae} is a feed-forward \ac{nn}, which consists of an encoder and a decoder.
It learns efficient (non-correlated) encodings of the input data \cite{autoencoder2020}.
It is \textit{undercomplete} because the dimensionality of the hidden layer, or so-called hidden space, 
is lower than the dimensionality of the input layer \cite{seminar_ies}.
\textit{Feed-forward} means that the information flows from the input layer to the output layer \cite{seminar_ies}.
However, while training, the network employs backpropagation to update the parameters of the network \cite{seminar_ies}.

The \ac{ae}'s goal is to approximate the identity function $f_\theta(X) = X$ (trivial solution eliminated) for input $X$ and 
function parameters to be learned $\theta$ \cite{seminar_ies}.
The input and output layers have the same dimensionality.

% \begin{equation}
%     Z = f_E(W_\theta X + B_\theta)
%     \label{eq:encoder}
% \end{equation}

% The formulae for the encoder and the decoder are given in \autoref{eq:encoder} and in \autoref{eq:decoder}.
% The parameter $W_\theta$ is the weight, whereas $B_\theta$ is the bias.
% The activation functions $f_E$ and $f_D$ are possibly non-linear and thus, the \ac{nn} is capable of more than linear regression.
% $Z$ is the low-dimensional representation of the input $X$ and $X'$ is the reconstructed version of $Z$.

% \begin{equation}
%     X' = f_D(W_\theta Z + B_\theta)
%     \label{eq:decoder}
% \end{equation}

% The loss function $L$ is defined as the reconstruction error between the input $X$ and the output $X'$ \cite{seminar_ies}.
% In order to train the \ac{ae}, the loss function is minimized \cite{autoencoder2021}.


\input{chapter/section-03/eigenfaces}