\section{Compression of data}\label{sec:compression}

According to \citeauthor{clusteringDocs2020}, a decomposition of data preserves the inner structure in inherent clusters. 
When data analysis techniques are applied to reasonably low-dimensional data, the results usually improve.
Moreover, compressed data is less memory-consuming and often less difficult to interpret by humans 
since there are more methods to visualize low-dimensional data.
In the following, two approaches to reduce the dimensionality of data are presented.

\subsection{\acl*{ae}}\label{subsec:autoencoder}

The idea of this approach is to find a meaningful low-dimensional version of the input.
The high-dimensional data is encoded into a low-dimensional representation using the encoder of an undercomplete \ac{ae} \cite{autoencoder2020}.
Hence, the output of the latent space corresponds to the input's embedding. 
The low-dimensional representation can be decoded into an approximation of the high-dimensional original using the decoder of the \ac{ae}.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.4\textwidth]{images/embeddings/autoencoder/AE.svg}
    \caption[Structure of an \ac{ae}]
    {Structure of an \acs*{ae} cf. \cite{autoencoder2020}.
    The six-dimensional input is encoded into a three-dimensional representation.
    This encoding is decoded into a six-dimensional approximation of the original input.}
    \label{fig:ae}
\end{figure}

An undercomplete \ac{ae} is a feed-forward \ac{nn}, which consists of an encoder and a decoder.
\acp{nn} are discussed in \autoref{sec:neural_network}.
It learns efficient (non-correlated) encodings of the input data \cite{autoencoder2020}.
It is \textit{undercomplete} because the dimensionality of the hidden layer, or so-called hidden space, 
is lower than the dimensionality of the input layer \cite{seminar_ies}.
The input and output layers have the same dimensionality.

The network employs backpropagation to update the parameters of the network during training.
The \ac{ae}'s goal is to approximate the identity function $f_\theta(X) = X$ (trivial solution eliminated) for input $X$ and 
function parameters to be learned $\theta$ \cite{seminar_ies}.

\input{chapter/section-03/eigenfaces}