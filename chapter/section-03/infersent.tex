\subsection{\infersent{}}\label{subsec:inferSent}

% general
\infersent{} is a sentence embedding method trained in a supervised manner on the \ac{snli} dataset \cite{inferSent2018, HfsentTrans2019}.
The trained model is transferable to other tasks.
% Bi-LSTM as sentence encoder
\citeauthor{inferSent2018} compare multiple architectures in their work.
The \ac{bilstm} architecture with max pooling which was found to be the best option for the sentence encoder 
is depicted in \autoref{fig:infersent_bilstm} \cite{inferSent2018}.

% LSTM
A \ac{lstm} is a \ac{rnn} that is capable of learning long-term dependencies.
\acp{rnn} have closed loops, i.e.\ feedback connections between the nodes \cite{rnn_book2001}.
In other words, 
a \ac{lstm} is able to remember information as a so-called \textit{state}.
Certain \ac{lstm} mechanisms control whether the current state is deleted, whether new data is saved and 
to what degree the current state contributes to the current input processed in the node.
Hence, \ac{lstm} nodes are not only influenced by former outputs but also by their state.
Since the \ac{lstm} computes different numbers of hidden vectors $h_t$ depending on the length of a sentence, 
a max pooling layer is applied to the hidden vectors which selects the maximum value for a patch of the hidden vectors.

\begin{figure}[!htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.9\textwidth]{images/embeddings/infersent/Infersent.svg}
    \caption[Architecture of \infersent{}]{Architecture of the \acs*{bilstm} model with max pooling used for \infersent{} cf. \cite{inferSent2018}.
    The input sentence $(w_1, w_2, ..., w_T)$ is read from both directions by a forward and a backward \acs*{lstm} 
    producing $\overrightarrow{h_t}$ and $\overleftarrow{h_t}$ respectively.
    After concatenating $\overrightarrow{h_t}$ and $\overleftarrow{h_t}$ to $h_t$, max pooling is applied.
    The output is a fixed-sized embedding.
    }
    \label{fig:infersent_bilstm}
\end{figure}

% Bi-LSTM
According to \citeauthor{HfsentTrans2019}, \infersent{} consists of a single \ac{bilstm} layer \cite{HfsentTrans2019}.
Given a sentence $(w_1, w_2, ..., w_T)$ of $T$ words, the \ac{bilstm} architecture computes the hidden representations $h_t$ for each word $w_t$.
The hidden representation $h_t$ is the concatenation of the forward and backward hidden vectors $\overrightarrow{h_t}$ and $\overleftarrow{h_t}$.
$\overrightarrow{h_t}$ and $\overleftarrow{h_t}$ are produced by a forward and backward \ac{lstm} respectively.
Hence, the sentence is read from both directions and thus, considers past and future context.