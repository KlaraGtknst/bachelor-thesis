\subsection{\acl*{use}}\label{subsec:univ-sent-encoder}

\citeauthor{UniversalSentEnc2018} have published their \ac{use} model on TensorFlow Hub.
They propose two architectures, one based on a Transformer and one based on a \ac{dan}.
Both models' input is a lowercase tokenized string.
Their output is a 512-dimensional vector.

% Transformer
The transformer model is more accurate and more complex than the \ac{dan} model \cite{UniversalSentEnc2018}.
The transformer's (self) attention is used to compute context-aware word embeddings, which consider both the word order and their semantic identity.
Since a sequence of word embeddings of a sentence produces embeddings of different dimensions, the approach postprocesses the word embeddings.
A sentence vector is obtained by computing the element-wise sum of the word embeddings 
and normalizing the result by dividing by the square root of the sentence length.

% DAN
The \ac{dan} model receives real-valued embeddings of words and bi-grams as input.
Those input vectors can be obtained from the text strings using models such as the \ac{bow} model \cite{UniversalSentEnc-dan-input-emb}.
The embeddings are averaged and subsequently passed to a feedforward \ac{dnn} \cite{UniversalSentEnc2018}.
The architecture of the \ac{dan} model is depicted in \autoref{fig:use_dan}.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.9\textwidth]{images/embeddings/universal_sentence_encoder/DAN.svg}
    \caption[Architecture of \acs*{use}]{Architecture of the \acs*{dan} model used for \acs*{use} based on the textual description from \cite{inferSent2018}.
    The input words and bi-grams $(w_1, w_2, ..., w_N)$ are embedded.
    The embeddings are averaged and subsequently passed to a feedforward \acs*{dnn}, which produces a 512-dimensional sentence embedding.
    }
    \label{fig:use_dan}
\end{figure}

% dataset
The models are trained on both unsupervised training data, e.g., Wikipedia, and a supervised training dataset, i.e.\ \ac{snli} \cite{UniversalSentEnc2018, HfsentTrans2019}.
The unsupervised training task is to predict the context given an input, i.e.\ Skip-Gram like tasks.
The supervised training task is classification \cite{UniversalSentEnc2018}.

% complexity
%The transformer model is more complex than the \ac{dan} model.
% The transformer model complexity is $O(n^2)$, whereas the \ac{dan} model complexity is $O(n)$, 
% $n$ being the number of words in the sentence \cite{UniversalSentEnc2018}.
% % memory usage
% The memory usage of both models is equivalent to their complexity.
% \citeauthor{UniversalSentEnc2018} state that \ac{dan}'s memory usage is dominated by the parameters used to store the embeddings of the uni- and bi-grams.
% Moreover, the transformer model only stores the uni-gram embeddings and thus, can require less memory than \ac{dan} for short sentences \cite{UniversalSentEnc2018}.