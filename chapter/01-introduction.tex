\chapter{Introduction}\label{ch:introduction}

According to \cite{data-corpus-bahamas-leaks}, the Bahamas leak is roughly 38 GB collection of documents, which were leaked from in 2016.
The data is used by (German) tax offices to identify tax evasion.
However, it has proven to be challenging to identify the relevant documents and connections between documents due to the amount of documents in the leak.

Therefore, the goal of this thesis is to suggest approaches to support the investigators of the tax offices.
Text exploration methods include topic modelling.

The topics to be identified can be groups of words which appear more often than the average or groups of similar documents.
Hence, a topic is not always the defined topic in terms of content, but sometimes a statistical phenomenon.
Since different methods define different topics, as they work and define the meaning of 'topic' differently, 
their results are compared and evaluated on the dataset.

Besides literature research, application and evaluation of the methods identified, 
certain preprocessing methods have proven to be eminent to successful work with unstructured text data.
These methods include chunking/ tokenization (separating texts into equally sized segments), lemmatization (e.g., faster to fast), 
conversion to small letters and stop-word-lists.

\section{Motivation/ Objective}\label{sec:motivation}

Assumption: similarities between documents (in terms of appearance and content wise)
On a broader scope this thesis aims to provide computational means to facilitate the work with large unstructured text data for individuals.
In the following, certain goals are defined, which are to be achieved in this thesis.

Motivation/ problem: actively use machine learning techniques to analyse large text corpus and thus, reduce the amount of manual (human) work.
This includes analysis in terms of textual (content) and visual (appearance/ layout) information, like a human would do.
The goal is to identify similarities between documents and group (cluster) them together - topic of the cluster do not have to be labeled specificially.
This serves as a first step/ preprocessing, e.g., a human finds a document of interest (for instance from random sampling) and wants to find similar documents to it.


\begin{description}
    \item[Usability.]
    The methods should be bundeled in an application, which is easy to use and does not require any programming skills.
    \item[Semantic similarity.]
    The documents grouped together should be semantically similar.
    \item[Topic identification.]
    The topics identified should be meaningful to the task at hand.  
    \item[Offline Calculation.]
    The database should be calculated offline, so that the queries can be executed with little latency.
\end{description}


\section{Research Questions}\label{sec:research-questions}
The following research questions build the guideline for this thesis.

\subsection[\acs{rq}1]{\ac{rq}1: Effect of different preprocessing pipelines on performance?}\label{subsec:rq1}
In terms of \ac{rq}1, one could compare different types of stemmers (i.e. algorithmic vs. dictionary-based).

\subsection[\acs{rq}2]{\ac{rq}2: Effect of different similarity measurement types on performance?}\label{subsec:rq2}
In terms of \ac{rq}2, one could compare different types of similarity measurement types (i.e. cosine similarity vs. soft cosine similarity).

\subsection[\acs{rq}3]{\ac{rq}3: Which type of database is best suited for this task?}\label{subsec:rq3}
In terms of \ac{rq}3, one could compare different types of databases (i.e. object-orientated, relational, document).

\subsection[\acs{rq}4]{\ac{rq}4: Effect of different embeddings on performance?}\label{subsec:rq4}
In terms of \ac{rq}4, one could compare different types of embeddings (i.e. Doc2Vec, Bag-of-words, \ac{lda}, BERTopic).



\section{Structure of the Thesis}\label{sec:structure-of-the-thesis}
The rest of this thesis is structured as follows.
Chapter \ref{ch:methodology} provides background information on the topic of this thesis.
Chapter \ref{ch:implementation} describes the implementation of the methods.
Chapter \ref{ch:evaluation} evaluates the methods.
Chapter \ref{ch:results} discusses the results.
Chapter \ref{ch:conclusion} concludes this thesis and 
Chapter \ref{ch:outlook} gives an outlook on future work.