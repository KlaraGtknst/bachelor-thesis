\subsubsection*{\ac{sbert}}\label{subsubsec:impl-sbert}

The \ac{sbert} model is implemented with PyTorch \cite{HfsentTrans2019}.
An instance of the model is obtained as shown in \lst{lst:impl-sbert}.
The model contains a \ac{bert} transformer, which has a \texttt{max\_seq\_length} of $128$. 
It does not convert inputs to lowercase by default \cite{sbert-dev}.
The output of the \ac{bert} transformer is passed to a pooling layer, which is initialized with the \texttt{pooling\_mode} parameter.
The default is \texttt{mean\_pooling}, which calculates the mean of the output vectors of the transformer.
The other options include \texttt{cls\_token\_pooling}, which returns the output of the first token and 
\texttt{max\_pooling}, which returns the maximum value of the output vectors.
%and \texttt{pooling\_mode\_mean\_sqrt\_len\_tokens}.
The word embedding dimension is 384 by default \cite{sbert-dev}.

\begin{listing}[htp]
    \begin{minted}{python3}
        SentenceTransformer('paraphrase-MiniLM-L6-v2')
    \end{minted}
    \caption[Initialization of the \acs*{sbert} model]{Initialization of the \acs*{sbert} model.
    }
    \label{lst:impl-sbert}
\end{listing}