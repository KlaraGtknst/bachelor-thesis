\chapter{Evaluation}\label{ch:evaluation}
Parameters of models
\input{chapter/section-04/eval-similarity-measurements.tex}

\input{chapter/section-04/eval-eigendocs.tex}

\input{chapter/section-04/eval-autoencoder.tex}

\input{chapter/section-04/eval-optics.tex}

\input{chapter/section-04/elasticsearch_db.tex}

\input{chapter/section-04/eval-tfidf.tex}

\input{chapter/section-04/eval-doc2vec.tex}

\input{chapter/section-04/eval-infersent.tex}

\input{chapter/section-04/eval-universal-sent-enc.tex}





\section{analysis/ comparison of models}\label{sec:evaluation-models}

% parameters
Similar to \citeauthor{glove2014}'s work, in this work, for many models used, any unspecified parameters are set to their default values, 
assuming that they are close to optimal
acknowledging that this simplification should be revised in a more thorough analysis.

% comparing models (qualitative)
difference query responses for different models?
any images which produce unusual results?

\section{Evaluation of the performance}\label{sec:evaluation-performance}

\subsection{Fahnder clustern}\label{subsec:evaluation-metric1}

\subsection{Fahnder bewerten Resultate (image matrix)}\label{subsec:evaluation-metric2}

\section{Evaluation of the usability}\label{sec:evaluation-usability}

\subsection{Metrics}\label{subsec:evaluation-metrics}