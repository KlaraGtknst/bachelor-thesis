\chapter{Own approach}\label{ch:implementation}

In this thesis, a tool is developed that offers text queries, detailed document inspection and queries for semantically or visually similar documents to the user.
This chapter describes how the theoretical basics from \autoref{ch:methodology} interplay and how they are used to construct this tool.
\autoref{sec:offline-processing} outlines the steps carried out before the application is operative, 
\autoref{sec:ui} covers the resulting application and 
\autoref{sec:trade-off} discusses the dilemma faced when balancing memory usage and query time. 
Specific parameter choices are explained in \autoref{ch:evaluation}.


\section{Offline Processing}\label{sec:offline-processing}

The tasks carried out before the application is operative are outlined in this section.
They are considered to be offline preprocessing steps.  
A process works in an offline fashion if the process requires access to the whole data at once \cite{offline2022}.
This section outlines implementation details of the way the data is derived, 
the database storing the data and the baseline topic analysis approach compared to this work's application.

 % Database
\input{chapter/section-04/impl-elasticsearch.tex}

% Eigendocs
\input{chapter/section-04/eigendocs.tex}

% Embeddings
\subsection{Embeddings}\label{subsec:impl-embeddings}

Firstly, the implementation of the \ac{ae} used to compress high-dimensional embeddings is presented.
Then, the models used to encode the textual data are outlined below with regard to implementation details.

\input{chapter/section-04/impl-autoencoder.tex}

\input{chapter/section-04/impl-tfidf.tex}

\input{chapter/section-04/impl-doc2vec.tex}

\input{chapter/section-04/impl-infersent.tex}

\input{chapter/section-04/impl-univ_sent_enc.tex}

\input{chapter/section-04/impl-hf-sbert.tex}

% Clustering
\input{chapter/section-04/clustering_optics.tex}

% topic analysis
\subsection{Topic analysis}\label{impl-topic-modeling}

Two topic analysis approaches are outlined below.
The first one serves as the baseline model of this thesis and 
the second one is used multiple times throughout the application to visualize results obtained by queries.


\input{chapter/section-04/impl-top2vec.tex}

\input{chapter/section-04/impl-wordcloud.tex}

% Slurm/ Server
\input{chapter/section-04/slurm.tex} % last subsection

% UI
\input{chapter/section-04/user_interface.tex}

\newpage
\section{Trade-off between memory and query time}\label{sec:trade-off}

At the beginning of this thesis, it was unclear to what degree the tool, 
i.e.\ the database fields and query results should be precomputed.
A tool which is trained once offline is beneficial due to the amount and the nature of the data.
The Bahamas leak is static and thus, the database does not need to be updated with new documents.
%Since \databaseName{} provides fast queries, it is sufficient to rely on real-time queries.

In the course of filling the database with information, 
one had to face obstacles not only regarding excessive memory usage but also long run times of methods.
Early on it became evident that one either had to reduce accuracy and details in order to achieve less memory or 
one had to settle for minutes to hours of calculations and bigger costs in terms of memory consumption.

Beforehand, it was not clear which information, i.e.\ fields in the database, seemed worth the time and memory.
For instance, initially, the image of the first PDF page of each document was saved alongside the other fields within the database.
After scaling the amount of data stored in the database to about 2900 documents, 
this approach caused severe issues in terms of memory usage.
Hence, this field is omitted.