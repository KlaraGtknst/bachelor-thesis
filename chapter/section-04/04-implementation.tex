\chapter{Implementation}\label{ch:implementation}

This chapter describes how the theoretical basics from \autoref{ch:methodology} interplay and how they are used in this thesis.
In this thesis, a tool is developed that offers text queries, detailed document inspection and queries for semantically or visually similar documents to the user.
\autoref{sec:offline-processing} outlines the steps carried out before the application is operative, 
\autoref{sec:ui} is about the resulting application and 
\autoref{sec:trade-off} discusses the dilemma faced when balancing memory usage and query time. 
Specific parameter choices are explained in \autoref{ch:evaluation}.


\section{Offline Processing}\label{sec:offline-processing}
This section outlines implementation details of the data fed into the database, the database itself and the baseline topic analysis approach compared to this work's application.
 % Database
\input{chapter/section-04/impl-elasticsearch.tex}

% Eigendocs
\input{chapter/section-04/eigendocs.tex}

% Embeddings
\subsection{Embeddings}\label{subsec:impl-embeddings}
The models used to encode the textual data from the data corpus are outlined below with regard to implementation details.

\input{chapter/section-04/impl-tfidf.tex}

\input{chapter/section-04/impl-doc2vec.tex}

\input{chapter/section-04/impl-infersent.tex}

\input{chapter/section-04/impl-univ_sent_enc.tex}

\input{chapter/section-04/impl-hf-sbert.tex}

\input{chapter/section-04/impl-autoencoder.tex}

% Clustering
\input{chapter/section-04/clustering_optics.tex}

% topic analysis
\subsection{Topic analysis}\label{impl-topic-modeling}

Two topic analysis approaches are outlined below.
The first one serves as the baseline model of this thesis and 
the second one is used multiple times throughout the application to visualize results obtained by queries.


\input{chapter/section-04/impl-top2vec.tex}

\input{chapter/section-04/impl-wordcloud.tex}

% Slurm/ Server
\input{chapter/section-04/slurm.tex} % last subsection

% UI
\input{chapter/section-04/user_interface.tex}


\section{Trade-off between memory and query time}\label{sec:trade-off}

At the beginning of this thesis, it was unclear to what degree the tool, 
i. e. the database fields and query results should be precomputed.
A tool which is trained once offline is beneficial due to the amount of data.
Since \databaseName{} provides fast queries, it is sufficient to rely on real-time queries.

In the course of filling the database with information, 
one had to face obstacles not only regarding excessive memory usage but also long run times of methods.
Early on it became evident that one either had to reduce accuracy and details in order to achieve less memory or 
one had to settle for minutes to hours of calculations and bigger costs in terms of memory consumption.

Beforehand, it was not as clear which information, i. e. fields in the database, seemed worth the time and memory.
For instance, initially, the image of the first \ac{pdf} page of each document was saved alongside the other fields within the database.
After scaling the amount of data stored in the database to about 2900 documents, 
this approach caused severe issues in terms of memory usage.
Hence, this field is omitted.