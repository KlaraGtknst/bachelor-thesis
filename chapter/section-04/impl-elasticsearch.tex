\subsection{Database}\label{subsec:impl-db}
First, the content of the \databaseName{} database is described, then, the initialization, insertion and updating process of filling the database are explained 
and finally, the process of querying is outlined.

% content
\subsubsection*{Content of the database}
In this work, the database is filled once with data from a large unstructured corpus of \ac{pdf} files.
After the initialization of the database, it is used for queries. 
Therefore, the workflow of processing the text corpus is done completely offline and in advance.

The index \textit{Bahamas} stores different embeddings of the information derived from the text layer and metadata of the documents.
As depicted in \autoref{fig:pdf2db}, not only textual information is stored in the database, 
but also information about the appearance of the first page of the \ac{pdf}.
The structure of the index is presented in \autoref{tbl:Elasticsearch-fields}.

\begin{table}[]
    \caption{Fields of the \databaseName{} database index \textit{Bahamas}.}
    \begin{tabular}{|
    >{\columncolor[HTML]{EFEFEF}}l |p{0.63\textwidth}|}
    \hline
    \cellcolor[HTML]{C0C0C0}\textbf{Field name} & \cellcolor[HTML]{C0C0C0}\textbf{Field description}                                     \\ \hline
    \_id                                        & Unique identifier of document \texttt{i}. The identifier is generated by the sha256 hash algorithm from hashlib using the \ac{pdf} file as input.\\ \hline
    doc2vec                                     & 55 dimensional \ac{d2v} embedding of \texttt{i}.                                                          \\ \hline
    sim\_docs\_tfidf                            & \ac{tfidf} embedding + all-zero flag of \texttt{i}. The all-zero flag is one if the \ac{tfidf} embedding consists of only zeros, zero else. If the embedding's dimensionality is greater than 2048, the encoder of a trained \ac{ae} is used to compress the embedding.\\ \hline
    google\_univ\_sent\_encoding                & 512 dimensional \ac{use} embedding of \texttt{i}.                                     \\ \hline
    huggingface\_sent\_transformer              & 384 dimensional \ac{sbert} embedding of \texttt{i}.                                  \\ \hline
    inferSent\_AE                               & \infersent{} embedding of \texttt{i}. Since the pretrained \infersent{} model embedding's dimension is 4096, the encoder of a trained \ac{ae} has to reduce the dimension to 2048.                                                    \\ \hline
    pca\_image                                  & 13-dimensional \ac{pca} version of first page image of \texttt{i}.                      \\ \hline
    pca\_optics\_cluster                        & Cluster of \texttt{i} identified by \acs{optics} on \ac{pca} version of image.            \\ \hline
    argmax\_pca\_cluster                        & Number of maximum \ac{pca} component as cluster of \texttt{i}.                            \\ \hline
    text                                        & Text of \texttt{i}.                                                                       \\ \hline
    path                                        & Path to \texttt{i}.                                                     \\ \hline
    \end{tabular}
    \label{tbl:Elasticsearch-fields}
\end{table}

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=1\textwidth]{images/Elasticsearch/PDFs_to_database}
    \caption[Database procedure]{\acp{pdf} to Database. 
    First, the data is preprocessed:
    The first page of a \ac{pdf} file is converted to an image and the complete text is extracted. 
    The images are stored in the database as well as the text and different embeddings of the text.
    Some values, such as the image or the \infersent{} embedding, have to be compressed to become a vector of at most 2048 dimensions.
    }
    \label{fig:pdf2db}
\end{figure}

% initialize, insert, update
\subsubsection*{Initialization, insertion and updating}
To facilitate working with and running the code the initialization of the database is split into multiple steps.
As depicted in \autoref{fig:init_db}, first the database is initialized by defining the index name and the mappings, i.e.\ the field names, types and sizes.
This step is carried out using the method \texttt{create}.

\begin{figure}[!htb] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.5\textwidth]{images/Elasticsearch/init_db.svg}
    \caption[Initialization and filling of the database]{Procedure of initialization and filling of the database.}
    \label{fig:init_db}
\end{figure}

Afterwards, the documents are created using the method \texttt{create}.
The initial creation of a document only defines the fields \texttt{id}, \texttt{text} and \texttt{path}.
% In order to maximise efficiency when updating the database, the \databaseName{}'s built-in functionality \texttt{bulk} is used.
% \texttt{bulk} sends chunks of multiple requests to the database.
% As displayed in \lst{lst:db_bulk}, \texttt{bulk} is called with the \databaseName{} client, 
% a function which yields requests and parameters, which define the return values.
% The structure of a function that yields requests is shown in \lst{lst:db_bulk_yield}.

% \begin{listing}[htp]
%     \begin{minted}{python3}
%         bulk(client, create_document_aux(src_paths, client), stats_only= True)
%     \end{minted}
%     \caption[Usage of \databaseName{}'s helper functionality \texttt{bulk}]
%     {Usage of \databaseName{}'s helper functionality \texttt{bulk} to send multiple requests to the database in chunks.
%     }
%     \label{lst:db_bulk}
% \end{listing}

% \begin{listing}[htp]
%     \begin{minted}{python3}
%         def create_document_aux(src_paths: list, client: Elasticsearch):  
%             for path in src_paths:           
%                 id = get_hash_file(path)
%                 if get_doc_meta_data(client, doc_id=id) is not None:
%                     continue
%                 text = pdf_to_str(path)
%                 yield { '_op_type': 'create',
%                         '_index': 'bahamas',
%                         '_id': id,
%                         "text": text,
%                         "path": path}
%     \end{minted}
%     \caption[Method that yields requests for \texttt{bulk}]
%     {Method that yields requests for \texttt{bulk}.
%     The method checks if the document is already in the database and if not, it yields a request to create the document.
%     }
%     \label{lst:db_bulk_yield}
% \end{listing}

The embeddings are added to the documents in a third step.
% To make sure that it is possible to update embeddings individually without changing other fields, 
% a method to insert the embeddings of a specific model for all documents is created.
% The documents are updated using the \texttt{update} keyword and \texttt{bulk}.
To increase the efficiency of this step, data parallelism, i.e.\ parallelizing the execution of a method across multiple input values, is applied.
In this work, the data to be split among multiple processes is a set of paths to documents.
The \texttt{Pool} object from the multiprocessing module is used for data parallelism.
The steps carried out are displayed in \lst{lst:db_Pool_embeddings}.
First, the absolute paths of all documents are saved in a list.
This list is divided in \texttt{num\_cpus} many sublists of similar size.
Each process works on a sublist.
The embeddings are subsequentially inserted into the database for each sublist. 

\begin{listing}[htp]
    \begin{minted}{python3}
        with Pool(processes=num_cpus) as pool:
            for model_name in model_names:
                proc_wrap = wrapper(model_name=model_name, baseDir=src_path)
                pool.map(proc_wrap, sub_lists)
    \end{minted}
    \caption[Usage of \texttt{Pool} for data parallelism]
    {Usage of \texttt{Pool} for data parallelism.
    The paths to the documents to insert are divided into sublists which are simultaneously inserted into the database.
    Since the \texttt{Pool} object does not work with a \texttt{lambda} function, 
    a class \texttt{wrapper} is created which provides the same functionality.
    }
    \label{lst:db_Pool_embeddings}
\end{listing}

The document embeddings are added to the database using the method \texttt{update} as displayed in \lst{lst:db_Pool_update}.

\begin{listing}[htp]
    \begin{minted}{python3}
        client.update(index='bahamas', id=id, body={'doc': 
            {MODELS2EMB[model_name]: embedding}})
    \end{minted}
    \caption[Update of a database entry]
    {Update of a database entry to insert a specific embedding.
    }
    \label{lst:db_Pool_update}
\end{listing}


% search
\subsubsection*{Queries}
The default analyzer is used for the full-text search since for instance configuring a maximum token length did not seem necessary or likely to improve the results.

\begin{listing}[htp]
    \begin{minted}{python3}
        results = elastic_search_client.search(
            index='bahamas', 
            size=count,
            from_=(page*count),
            query= {'match': {
                        'text': {'query':text,
                                'fuzziness': 'AUTO',}
                    }, 
                }, source_includes=SRC_INCLUDES)
    \end{minted}
    \caption[Query to an \databaseName{} database index]{Exemplary query to an \databaseName{} database index.
    The parameters \texttt{size} and \texttt{from\_} define the number of results to return and the start index of the results.
    To enable fuzzy search a value for \texttt{fuzziness} has to be set. 
    }
    \label{lst:fuzzy_query}
\end{listing}

Moreover, the fuzzy matching option is set to \texttt{AUTO}, which means in terms of keyword or text fields that the allowed Levenshtein Edit Distance, 
i.e.\ number of characters changed to create an exact match between two terms, to be considered a match, is correlated to the length of the term \cite{Elasticsearch-fuzziness}.
By default, terms of length up to two characters must match exactly, terms of length three to five characters must have an edit distance of one and 
terms of length six or more characters must have an edit distance of two \cite{Elasticsearch-fuzziness}.
An exemplary query, which uses fuzzy search is given in \lst{lst:fuzzy_query}.

According to \citeauthor{Elasticsearch-kNN-HNSW}, one of \ac{knn} search's use cases is semantic document retrieval, which makes it a good fit for this task.
In this work, the approximate nearest neighbours search is used, since it is faster and the results are good enough for the purpose of this work.
The similarity measure used in this work is the cosine similarity, which calculates the \texttt{\_score} of a document according to \autoref{eq:cosine-similarity-db} from \cite{Elasticsearch-kNN-similarity}, 
where \texttt{query} is the query vector and \texttt{vector} is the vector representation of the document in the database.
The other similarity measures provided by \databaseName{} are \texttt{l2\_norm} or 
so-called Euclidian distance and \texttt{dot\_product} which is the non-auto-normalized version of the \texttt{cosine} option.
Since cosine is not defined on vectors with zero magnitude, embeddings that can return all zero vector representations, such as \ac{tfidf}, 
are enhanced with an all-zero flag before inserting them into the database.

\begin{equation}
    \frac{1 + \text{cosine}(\text{query}, \text{vector})}{2}
    \label{eq:cosine-similarity-db}
\end{equation}

In this work, the only tool from the elastic stack used is \databaseName{}.
Without Kibana, the used models are saved on disk as \ac{pkl} files.
Consequently, instead of using the \ac{knn} query structure for semantic search on embeddings provided by \databaseName{}, the normal \ac{knn} search on a field that contains an embedding is used.