\subsection{\ac{d2v}}\label{subsec:doc2vec}

Another term used for \ac{d2v} is \textit{Paragraph Vector} \cite{clusteringDocs2020, SentRep2014}.
\ac{d2v} addresses the problems of \ac{tfidf} by encoding texts as $n-$dimensional vectors learnt using the words' context \cite{clusteringDocs2020}.
Hence, it preserves semantic similarities between words and encodes many linguistic regularities and patterns \cite{SkipGram2013}.
According to \citeauthor{clusteringDocs2020} and \citeauthor{SentRep2014}, 
\ac{d2v} learns continuous distributed vector representations for pieces of the text.
The model handles inputs of different dimensions and thus, tokens are sentences, paragraphs or documents.

\ac{d2v} is an adaption of the \ac{w2v} model, which maps words into a \ac{vsm} under consideration of their semantic similarities \cite{clusteringDocs2020}.
The underlying hypothesis of both approaches is that words appearing in similar contexts are semantically similar \cite{clusteringDocs2020}.
The \ac{w2v} embedding is obtained using a \ac{nn} \cite{clusteringDocs2020}.
The \ac{nn} is shallow, i.e. has only one hidden layer.
This hidden layer creates the embedding of input data.
There are two approaches as to how to design the architecture of the \ac{nn}:
\begin{itemize}
    \item \ac{pvdm}: 
        Predicts a word given a context \cite{SentRep2014, WordRep2013}.
    \item \ac{pvdbow}: 
        Predicts the context given a word \cite{EmbDist2015, SkipGram2013, SentRep2014}.
\end{itemize}

% \begin{equation}
%     \frac{1}{T}\sum_{t=k}^{T-k}\text{log}(p(w_t | w_{t-k}, ..., w_{t+k}))
%     \label{eq:word2vec-cbow}
% \end{equation}

\begin{figure}%
    \centering
    \subfloat[\centering \ac{cbow} architecture cf. \cite{WordRep2013}.]
    {{\includesvg[width=7cm]{images/embeddings/doc2vec/CBOW.svg}}}%
    \qquad
    \subfloat[\centering \ac{pvdm} architecture cf. \cite{SentRep2014}.]
    {{\includesvg[width=6cm]{images/embeddings/doc2vec/PV-DM.svg} }}%
    \caption{Both approaches predict the centre word using the context.
    \ac{pvdm} is an adaption of \ac{cbow} to work on a set of documents or paragraphs instead of words.
    }%
    \label{fig:pvdm}%
\end{figure}
 
% context to centre word
The \ac{pvdm} extends the \ac{cbow} to work on a corpus of documents instead of on a set of words \cite{clusteringDocs2020}:
As usual, vectors representing the words are obtained using the \ac{cbow} model.
The \ac{cbow} has a single-layer architecture based on the inner product between two word vectors \cite{glove2014}.
%Given training words $ w_{1}, ..., w_{T}$, the objective of the \ac{w2v} model \ac{cbow} is to maximize the average log probability in 
%\autoref{eq:word2vec-cbow} from \cite{SentRep2014}.
The word vectors can be concatenated or averaged/ summed up \cite{SentRep2014}.
Each document is mapped to a vector using an additional document-to-vector matrix.
Both document and word vectors are initialized randomly, but trained to convey meaning in terms of semantic differentiation.
In order to train the model, centre words are predicted using the context, i.e. words within a sliding window, 
and their document, respectively represented as vectors \cite{SentRep2014}.
The document vector is added to incorporate the document's topic and thus, acts like a memory \cite{SentRep2014, Top2Vec2020}.
In the work of \cite{SentRep2014}, the document vector is concatenated to the word vectors.
The resulting vector is the prediction of the central word.
The approaches are displayed in \autoref{fig:pvdm}.
\textcolor{red}{prediction, loss function? B. in paper}

According to \cite{SentRep2014}, both vector(s) generating models are trained using stochastic gradient descent and backpropagation.
The authors of \cite{SentRep2014} also state that the document vectors are unique, while the word vectors are shared across the whole corpus. 

\begin{figure}%
    \centering
    \subfloat[\centering Skip-gram architecture cf. \cite{WordRep2013}.]
    {{\includesvg[width=7cm]{images/embeddings/doc2vec/Skip-gram.svg}}}%
    \qquad
    \subfloat[\centering \ac{pvdbow} architecture cf. \cite{SentRep2014}.]
    {{\includesvg[width=4.5cm]{images/embeddings/doc2vec/PV-DBOW.svg} }}%
    \caption{Both approaches predict the context.
    \ac{pvdbow} is an adaption of Skip-gram to work on a set of documents or paragraphs instead of words.
    }%
    \label{fig:pvdbow}%
\end{figure}
% centre word to context
The \ac{pvdbow} approach is the adaption of the \ac{w2v} algorithm Skip-Gram and predicts the context 
given (a centre word and) the paragraph/ document \cite{SentRep2014}.
The approaches are displayed in \autoref{fig:pvdbow}.
%According to \citeauthor{SentRep2014}, the 


% paper widerspricht sich
%According to \citeauthor{clusteringDocs2020}, the \ac{doc2vec} model's performance is influenced quality of the preprocessing.
%If for instance, the stemmer assigns words with different meaning to the same root, there is a degradation in performance.