\section{Embeddings}\label{sec:embeddings}

Usually, \ac{ml} techniques embeddings, such as K-Means, require the text input data to be converted to embeddings \cite{SentRep2014}.
Embeddings are numerical representations of words, sentences or texts.
They can be used to present the textual data as real-valued vectors in a \ac{vsm}.
\acp{vsm} are commonly used due to their conceptual simplicity and because spatial proximity serves as a metaphor for semantic proximity 
\cite{tfidf2008, UniversalSentEnc2018, HfsentTrans2019, Top2Vec2020}.
Representations in a vector space can improve the performance in \ac{nlp} tasks \cite{SkipGram2013}.
According to \citeauthor{tfidf2008}, when representing text the first step is indexing, i.e. assigning indexing terms to the document.
The second task is to assign weights to the terms which correspond to the importance of the term in the document.
The weights assigned depend on the method and the assumptions of the model chosen to carry out the assignments.

The following section outlines a selection of embeddings.
Let a corpus of documents be denoted $D= \left\{d_1, d_2, ..., d_M  \right\}$, the number of documents in the dataset $M = \left\| D \right\|$,
a sequence of terms $w_{ij}$ or so-called document $d_i = \left\{w_{i1}, w_{i2}, ..., d_{iV}  \right\}$, $V$ being the length of the vocabulary, 
i.e. set of distinct words, of the corpus of documents \cite{clusteringDocs2020}.


\input{chapter/section-02/tfidf.tex}

\input{chapter/section-02/doc2vec.tex}

\input{chapter/section-02/univ_sent_encoder.tex}

\input{chapter/section-02/infersent.tex}

\input{chapter/section-02/sent_transformers_hf.tex}