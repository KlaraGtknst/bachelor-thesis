\subsection{Hugging face's \acs{sbert}}\label{subsec:hf-sent-ransformers}

\ac{sbert} is an enhancement of \ac{bert}.
% BERT
\ac{bert} is a pre-trained transformer network.
It predicts a target value, for i.e. classification or regression tasks, based on two input sentences \cite{HfsentTrans2019}.
The input sentences are separated by a special token \texttt{[SEP]}.
The base-model applies multi-head attention over 12 transformer layers, whereas the large model applies multi-head attention over 24 transformer layers.
The final label is derived from a regression function, which receives the output of the 12th or 24th layer, respectively.
\citeauthor{HfsentTrans2019} state that \ac{bert} is not suitable for specific pair regression tasks, 
since the number of input sentence combinations is too big.
Another shortcoming of \ac{bert} is that it does not produce independent embeddings for single sentences.
More \citeauthor{HfsentTrans2019} found that common similarity measurements, for instance, the ones discussed in \autoref{sec:similarity-measurement}, 
do not perform well on the representations of sentences in a \ac{vsm} produced by \ac{bert} \cite{HfsentTrans2019}.

% SBERT
\ac{sbert} is a modification of \ac{bert} that provides fixed-sized embeddings for single sentences \cite{HfsentTrans2019}.
It consists of a siamese and triplet network architecture.
It differs from \ac{bert} in terms of architecture, since it adds a pooling layer after the \ac{bert} model.
The pooling strategies compared by \citeauthor{HfsentTrans2019} are using the output of the first/ \texttt{CLS} token, mean pooling and max pooling.

% training corpus
\ac{sbert} is trained on the \ac{snli} dataset.

% performance
According to \citeauthor{HfsentTrans2019}, \ac{sbert} outperforms \infersent{} and \ac{use}.