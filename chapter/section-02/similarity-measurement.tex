
\section{Similarity Measurement}\label{sec:similarity-measurement}

\cite{EmbDist2015}

\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

In the traditional bag-of-words approach the texts are represented as vectors of \ac{tfidf} coefficients \cite{soft_cosine2017}.
Without further processing, the vector is of size $N$, $N$ being the number of different words of the texts \cite{soft_cosine2017}.
Hence, a vector represents its corresponding text in a $N$-dimensional space.
This space is called \ac{vsm} \cite{soft_cosine2014}.

The similarity between two texts is measured by the cosine of the angle between their respective vectors \cite{soft_cosine2014}.
The cosine similarity is defined in \autoref{eq:cosine-similarity} from \cite{soft_cosine2014}.
$a \cdot b = \sum_{i=1}^{N}a_{i}b_{i}$ is the dot-product.
The dot-product is normalized with $\left\| x \right\| = \sqrt{x \cdot x}$ to unit Euclidean length \cite{soft_cosine2014}.
The cosine similarity is a value between $0$ and $1$ for positive values \cite{soft_cosine2014}.
According to \citeauthor{soft_cosine2014}, the formula has a time and space complexity of $O(N)$ for a pair of $N$-dimensional vectors.

\begin{equation}
    cosine(a,b) = \frac{a \cdot b}{\left\| a \right\| \times \left\| b \right\|} = \frac{\sum_{i=1}^{N}a_{i}b_{i}}{\sqrt{\sum_{i=1}^{N}{a}^2_{i}}\sqrt{\sum_{i=1}^{N}{b}^2_{i}}}
    \label{eq:cosine-similarity}
\end{equation}

The formula \autoref{eq:cosine-similarity} assumes that the vectors, which span the \ac{vsm} are orthogonal and thus, 
completely independent \cite{soft_cosine2014}.
However, in practical applications, this often is not the case \cite{soft_cosine2014}.


\subsection{Soft Cosine Similarity}\label{subsec:soft-cosine-similarity}

This similarity measure not only evaluates whether two texts consist of the same words but 
also takes into account the semantic (word-level) similarity or lexical relation of different words of the texts \cite{soft_cosine2017}.
Hence, it improves the shortcomings of the traditional cosine similarity measure, 
which assumes the tokens of the vocabulary are completely independent of each other \cite{soft_cosine2014}.

According to \citeauthor{soft_cosine2014}, in order to model this additional information, more dimensions are added to the \ac{vsm}.
These dimensions can be obtained, for instance, by multiplying the mean of two features of one vector with the similarity between them \cite{soft_cosine2014}.
The similarity can be calculated by using Levenshtein distance for e.g., n-grams, i.e. the number of operations necessary to convert one string into another, 
or using a dictionary of synonyms \cite{soft_cosine2014}.

Since this approach no longer assumes that different words are independent of each other, 
the basis vectors which span the \ac{vsm} are no longer orthogonal \cite{soft_cosine2014}.
The formula for the soft cosine similarity is defined in \autoref{eq:soft-cosine-similarity} from \cite{soft_cosine2014}.
The similarity $s_{ij}$ between the $i$-th and $j$-th basis vector is obtained using a similarity measure, such as synonymy \cite{soft_cosine2014}.

\begin{equation}
    soft\_cosine(a,b) = \frac{\sum_{i=1}^{N}\sum_{j=1}^{N}s_{ij}a_{i}b_{j}}{\sqrt{\sum_{i=1}^{N}\sum_{j=1}^{N}s_{ij}a_{i}a_{j}}\sqrt{\sum_{i=1}^{N}\sum_{j=1}^{N}s_{ij}b_{i}b_{j}}}
    \label{eq:soft-cosine-similarity}
\end{equation}

According to \citeauthor{soft_cosine2017}, the similarity between two texts is non-zero as soon as they share related words \cite{soft_cosine2017}.
If there is no similarity between different features, 
the soft cosine similarity from \autoref{eq:soft-cosine-similarity} is equal to the cosine similarity from \autoref{eq:cosine-similarity}.
The time and space complexity of the soft cosine similarity is $O(N^2)$ \cite{soft_cosine2014}.

In order to reduce the complexity, \citeauthor{soft_cosine2014} propose to use a sparse similarity matrix which only stores $s_{ij} > t$, 
$t$ being a threshold \cite{soft_cosine2014}.

\subsection{euclidian distance}\label{subsec:euclidian-distance}

%\subsection{Hamming distance}\label{subsec:hamming-distance}
