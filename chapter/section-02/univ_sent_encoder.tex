\subsection{\ac{use}}\label{subsec:univ-sent-encoder}

\citeauthor{UniversalSentEnc2018} have published their \ac{use} models on TF Hub.
They propose two models, one based on a Transformer architecture and one based on a \ac{dan}.
Both models' input is a lowercase tokenized string.
Their output is a 512-dimensional vector.

% Transformer
The transformer model is more accurate and more complex than the \ac{dan} model \cite{UniversalSentEnc2018}.
The transformer's attention is used to compute context aware word embeddings, which consider both the word order and their semantic identity.
Since the sequence of word embeddings of a sentence would produce embeddings of different dimensions, the approach postprocesses the word embeddings.
A sentence vector is obtained by computing the element-wise sum of the word embeddings 
and normalizing the result by dividing by the square root of the sentence length.

% DAN
The \ac{dan} model receives embeddings of words and bi-grams as input \textcolor{red}{Woher embeddings?}.
The embeddings are averaged and subsequently passed to a feedforward \ac{dnn}.

% dataset
The models are trained on both unsupervised training data, e.g., Wikipedia, and the supervised training dataset \ac{snli} \cite{UniversalSentEnc2018, HfsentTrans2019}.

% complexity
The transformer model is more complex than the \ac{dan} model.
More specifically, the transformer model complexity is $O(n^2)$, whereas the \ac{dan} model complexity is $O(n)$, 
$n$ being the number of words in the sentence \cite{UniversalSentEnc2018}.

% memory usage
The memory usage of both models is equivalent to their complexity.
\citeauthor{UniversalSentEnc2018} state that \ac{dan}'s memory usage is dominated by the parameters used to store the embeddings of the uni- and bi-grams.
Moreover, the transformer model only stores the uni-gram embeddings and thus, can require less memory than \ac{dan} for short sentences \cite{UniversalSentEnc2018}.

% final model
According to \citeauthor{HfsentTrans2019}, the transformer model is used.



