\chapter{Methodology}\label{ch:methodology}

Basic concepts, methods used, etc.

\section{Preprocessing}\label{sec:preprocessing}

\subsection{Tokenization/ Chunking}\label{subsec:tokenization}

\subsection{Lemmatization}\label{subsec:lemmatization}
Type of Stemmers. Porter, Snowball, Lancaster, etc.
% https://databasecamp.de/daten/stemming-lemmatizations for difference betwee stemmers and lemmatizers
Pre-trained/defined dense vector dictionaries (Word2Vec, \ac{glove}, FastText, etc.)

\subsection{Stop-Word-Removal}\label{subsec:stop-word-removal}

\subsection{Lower case}\label{subsec:lower-case}


\section{Similarity Measurement}\label{sec:similarity-measurement}

\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

\subsection{Soft Cosine Similarity}\label{subsec:soft-cosine-similarity}

\subsection{eucledian distance}\label{subsec:eucledian-distance}

\subsection{Hamming distance}\label{subsec:hamming-distance}

\subsection{\ac{wmd}}\label{subsec:word-mover-distance}

\subsection{SpaCy}\label{subsec:spacy}


\section{Embeddings}\label{sec:embeddings}

\subsection{\ac{cbow}}\label{subsec:bag-of-words}

\subsection{\ac{d2v}}\label{subsec:doc2vec}

\subsection{\ac{bertopic}}\label{subsec:bertopic}

\subsection{\ac{w2v}}\label{subsec:word2vec}

\subsection{\ac{tfidf}}\label{subsec:tfidf}

\subsection{One Hot Encoding}\label{subsec:one-hot-encoding}

\subsection{Skip Gram}\label{subsec:skip-gram}

\subsection{FastText}\label{subsec:fasttext}

\subsection{Annoy}\label{subsec:glove}


\section{Topic Modelling}\label{sec:topic-modelling}

\subsection{\ac{lda}}\label{subsec:latent-dirichlet-allocation}

\subsection{Word Clouds}\label{subsec:word-clouds}
frequency of words in a document

