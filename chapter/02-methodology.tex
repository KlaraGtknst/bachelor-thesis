\chapter{Methodology}\label{ch:methodology}

\cite{InformationRetrieval1999}

Basic concepts, methods used, etc.

\section{Preprocessing}\label{sec:preprocessing}

\subsection{Tokenization/ Chunking}\label{subsec:tokenization}

\subsection{Lemmatization}\label{subsec:lemmatization}
Type of Stemmers. Porter, Snowball, Lancaster, etc.
% https://databasecamp.de/daten/stemming-lemmatizations for difference betwee stemmers and lemmatizers
Pre-trained/defined dense vector dictionaries (Word2Vec, \ac{glove}, FastText, etc.)

\subsection{Stop-Word-Removal}\label{subsec:stop-word-removal}

\subsection{Lower case}\label{subsec:lower-case}


\section{Similarity Measurement}\label{sec:similarity-measurement}

\cite{EmbDist2015}

\subsection{Cosine Similarity}\label{subsec:cosine-similarity}

\subsection{Soft Cosine Similarity}\label{subsec:soft-cosine-similarity}

\subsection{euclidian distance}\label{subsec:euclidian-distance}

%\subsection{Hamming distance}\label{subsec:hamming-distance}

%\subsection{\ac{wmd}}\label{subsec:word-mover-distance}

%\subsection{SpaCy}\label{subsec:spacy}


\section{Embeddings}\label{sec:embeddings}

\cite{WordRep2013}
\cite{SentRep2014}

\textcolor{red}{Skizze von Pipeline f√ºr jedes Embedding, welche zeigt, wie die Daten vorverarbeitet (stemming etc.) werden/ was das Model selber macht.}

%\subsection{\ac{cbow}}\label{subsec:bag-of-words}

\subsection{\ac{d2v}}\label{subsec:doc2vec}
\cite{SentRep2014}
two flavor of doc2vec: PV-DM and PV-DBOW (https://thinkinfi.com/simple-doc2vec-explained/)
\cite{SkipGram2013}

%\subsection{\ac{w2v}}\label{subsec:word2vec}

\subsection{\ac{tfidf}}\label{subsec:tfidf}
Test test test
% svg does not icons
\begin{figure}[h] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=1.0\textwidth]{images/TFIDF_embedding}
    \caption{TFIDF Preprocessing}
    \label{fig:tfidf_embedding}
\end{figure}

\begin{figure}[htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=1.0\textwidth]{images/TFIDF_preprocessing}
    \caption{TFIDF Preprocessing}
    \label{fig:preprocessing}
\end{figure}

\subsection{Universal sentence encoder}\label{subsec:univ-sent-encoder}
\ac{use}
\cite{UniversalSentEnc2018}

\subsection{InferSent}\label{subsec:inferSent}
\cite{inferSent2018}

\subsection{Hugging face's sentence Transformers}\label{subsec:hf-sent-ransformers}
\cite{HfsentTrans2019}

%\subsection{One Hot Encoding}\label{subsec:one-hot-encoding}

%\subsection{Skip Gram}\label{subsec:skip-gram}
%rather PV-DBOW, it is the document version of skip gram
% https://github.com/RaRe-Technologies/gensim/issues/1925
%\cite{SkipGram2013}

%\subsection{FastText}\label{subsec:fasttext}




\section{Topic Modelling}\label{sec:topic-modelling}

\subsection{\ac{bertopic}}\label{subsec:bertopic}

\subsection{\ac{lda}}\label{subsec:latent-dirichlet-allocation}

\subsection{Word Clouds}\label{subsec:word-clouds}
frequency of words in a document


\section{Appearance of documents}\label{sec:appearance}
documents saved as images in .png format, bad quality to minimize size of database
when querying db, top image results looked similar, which is how idea of this section arose

\subsection{Compression of data}\label{subsec:compression}
\subsubsection{AE}\label{subsec:autoencoder}

\subsubsection{eigenface}\label{subsec:eigenface}
like pca, but for images

\subsection{Clustering}\label{subsec:clustering}

Clustering is used in a variety of domains to group data into meaningful subsclasses, i.e. clusters \cite{OPTICS2013, OPTICS2014, OPTICS_kMeans_2016}.
According to \citeauthor{OPTICS2013}, common domains include anomaly/ outlier detection, noise filtering, document clustering and image segmentation. 
The goal is to find clusters, which have a low inter-class similarity and a high intra-class similarity \cite{OPTICS2013}.
The similarity is measured by a distance function, which is dependent on the data type. 
Common distance functions are the Euclidean distance, the Manhattan distance and the Minkowski distance \cite{OPTICS_kMeans_2016}.

There are multiple clustering techniques, which can be divided into four categories \cite{OPTICS2016}: 
\begin{itemize}
    \item \textbf{Hierarchical clustering}:
    Algorithms, which create spherical or conex shaped clusters, possibly naturally occurring. 
    A terminal condition has to be defined beforehand.
    Examples include CLINK, SLINK \cite{OPTICS2014} and OPTICS \cite{OPTICS2013}.

    \item \textbf{Partitional based clustering}: 
    Algorithms, which partition the data into $k$ clusters, whereas $k$ is given apriori.
    Clusters are shaped in a spherical manner, are similar in size and not necessarily naturally occurring.
    KMeans is a popular example of a partitional based clustering algorithm.

    \item \textbf{Density based clustering}:
    Resulting clusters can be of arbitrary shape and size.
    The number of clusters does not have to be defined apriori.
    However, the algorithms are sensitive to input parameters, such as radius, minimum number of points and threshold.
    Popular examples are DBSCAN and OPTICS.
    
    \item \textbf{Grid based clustering}:
    Similar to density based clustering, but according to \citeauthor{OPTICS2016} better than density based clustering.
    Examples include flexible grid-based clustering \cite{OPTICS2014}.
    
\end{itemize}

Multiple approaches below use the term $\varepsilon$-neighbourhood, which is defined as the set of all objects within a certain distance $\varepsilon$ of a given object \cite{OPTICS2013}.
In other words: $N_\varepsilon (x) = \left\{ y \in X | dist(x,y) \le \varepsilon, y \neq x \right\}$ 


\subsubsection{KMeans}\label{subsec:kmeans}

The goal of KMeans is to partition the data/ $n \in \mathbb{N}$  objects into $k \in \mathbb{N}$  clusters, whereas $k$ is given apriori \cite{OPTICS_kMeans_2016}. %
First, $k$ centroids, i.e. cluster center, are randomly initialized.
Then, the objects are assigned to the closest centroid.
Afterwards, the centroids are updated by calculating the mean of the assigned objects.
The process is repeated until the terminating condition, for instance no more change in the clusters, is met \cite{OPTICS_kMeans_2016}.
By iteratively reassinging the objects to the closest centroid and updating the centroids, 
the algorithm minimizes the within-cluster sum of squared errors $E$, i.e. the sum of squared distances between objects in a cluster and their centroid, 
calculated in \autoref{eq:kmeans-error} from \cite{OPTICS_kMeans_2016}, 
where $C_{i}$ is the $i$-th cluster and $\mu_{i}$ is the mean of the objects in $C_{i}$.

\begin{equation}
    E = \sum_{i=1}^{k} \sum_{x \in C_{i}}\left\|x-\mu_{i}\right\|^{2}
\label{eq:kmeans-error}
\end{equation}

\citeauthor{OPTICS_kMeans_2016} claim, hat KMeans will not identfy outliers.


\subsubsection{DBSCAN}\label{subsec:dbscan}

The clusters identfied by DBSCAN have a high density, i.e. minimum number of objects within a certain distance of each other, and are seperated by low density regions \cite{OPTICS_kMeans_2016}.
In order to create clusters of minimum size and density, DBSCAN distinguishes between three types of objects \cite{OPTICS_kMeans_2016}:

\begin{itemize}
    \item \textbf{Core objects}: 
    An object with at least $minPts$ objects in its $\varepsilon$-neighbourhood $N_\varepsilon$.
    $N_\varepsilon$ contains all objects within radius $\varepsilon$ of $x$, $\varepsilon$ being the so-called generating distance \cite{OPTICS2013}.
    In other words: The neighbourhood of $x$ has to exceed a certain threshold for $x$ to be considered a core object, i.e. $| N_\varepsilon (x) | \geq minPts$ is true.

    \item \textbf{Border objects}: 
    An object with less than $minPts$ objects in its $\varepsilon$-neighbourhood, but is in the $\varepsilon$-neighbourhood of a core object.

    \item \textbf{Noise objects}: 
    An object, which is neither a core object nor a border object.
\end{itemize}

\begin{figure}[htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.2\textwidth]{images/density_reachable}
    \caption{Density reachability cf. \cite{OPTICS1999}.
    The point $y \in X$ is density reachable from $x \in X$, since there is a chain of objects $x_1, ..., x_n$ with $x_1 = x$ and $x_n = y$, 
    which are directly density reachable from each other.
    }
    \label{fig:density_reachable}
\end{figure}

\begin{figure}[htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.2\textwidth]{images/density_connected}
    \caption{Density connectivity cf. \cite{OPTICS1999}.
    The objects $x$ and $y$ are density connected, since there is a object $o$, from which both $x$ and $y$ are density reachable.
    }
    \label{fig:density_connected}
\end{figure}



\citeauthor{OPTICS_kMeans_2016} define $y$ is directly density reachable from $x$ as object $x$ is in the $\varepsilon$-neighbourhood of core object $y$ \cite{OPTICS_kMeans_2016}.
Moreover, a point $y \in X$ is density reachable from $x \in X$, if there is a chain of objects $x_1, ..., x_n$ with $x_1 = x$ and $x_n = y$, 
which are directly density reachable from each other \cite{OPTICS_kMeans_2016}.
The points $x$ and $y$ are said to be density connected, if there is a object $o$, from which both $x$ and $y$ are density reachable \cite{OPTICS_kMeans_2016}.
Density reachability is displayed in \autoref{fig:density_reachable} and density connectivity in \autoref{fig:density_connected}.

The DBSCAN algorithm starts by labeling all objects as core, border or noise points.
Then, it elimated noise points and links all core points, which are within each others neighbourhood \cite{OPTICS_kMeans_2016}.
Groups of connected core points form a cluster \cite{OPTICS_kMeans_2016}.
At the end every border point is assigned to the cluster of its corresponding core points \cite{OPTICS_kMeans_2016}.
The non-core point cluster assigning is non-deterministic \cite{OPTICS2013}.
This algorithm creates clusters as maximal set of density connected points \cite{OPTICS_kMeans_2016}.

According to \citeauthor{OPTICS_kMeans_2016}, DBSCAN can identify outlier or noise.
However, the algorithm is sensitive to the input parameters $minPts$ and $\varepsilon$ and has difficulties distinguishing closely located cluster \cite{OPTICS_kMeans_2016}.
Moreover, if one wants to obtain hiercharical clustering, one has to run the algorithm multiple times with different $\varepsilon$, which is expensive in terms of memory usage \cite{OPTICS2013}.



\subsubsection{HDBSCAN*}\label{subsec:hdbcan}


\subsubsection{OPTICS}\label{subsec:optics}

OPTICS does not return an explicit clustering, but rather a density-based clustering structure of the data, 
which is equivalent to clustering results of a broad range of parameters \cite{OPTICS1999}.
The idea of \citeauthor{OPTICS1999} is that real world datasets cannot be described by a single global density, since they often consist of different local densities, 
as displayed in \autoref{fig:diff_density_cluster}.

\begin{figure}[htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.5\textwidth]{images/diff_density_cluster}
    \caption{Clusters of different densities cf. \cite{OPTICS1999}.
    Since $C_1$ and $C_2$ have different densities than $A$ and $B$, a clustering algorithm using one global density parameter would detect the clusters $A$, $B$ and $C$, 
    rather than $A$, $B$, $C_1$ and $C_2$ .
    }
    \label{fig:diff_density_cluster}
\end{figure}

Opposed to DBSCAN, OPTICS is able to detect clusters of varying densities \cite{OPTICS2014}.
OPTICS produces an order of the elements according distance to the already added elements \cite{OPTICS2014, OPTICS2013}:
The first element added to the order list is arbitrary.
$\varepsilon$ defines the neighbourhood radius, i.e. the maximum distance between two elements, which are still considered to be in the same neighbourhood \cite{OPTICS_kMeans_2016}.
The order list is iteratively expanded by adding the element of the $\varepsilon$-neighbourhood of the order list items, which is has the smallest distance to any of the elements already in the order list.
Hence, clusters with higher density, i.e. lower $\varepsilon$, are added first (prioritized) \cite{OPTICS_kMeans_2016, OPTICS1999}.
When there are no more elements in the $\varepsilon$-neighbourhood to add, the process is repeated for the other clusters.
The non-core point cluster assigning is non-deterministic \cite{OPTICS2013}.

\begin{equation}
    RD(y) = \left\{
    \begin{array}{ll}
    \textrm{NULL} & \, \textrm{if |}N_\varepsilon (x)| < minPts \\
    max(core\_dist(x), dist(x,y)) & \, \textrm{otherwise} \\
    \end{array}
    \right. 
    \label{eq:optics-reachability-distance}
\end{equation}

OPTICS saves the reachability distance $RD(y)$, as calculated in \autoref{eq:optics-reachability-distance} from \cite{OPTICS2013},
with core\_dist being the minimal distance $\varepsilon^{min}$ such that $| N_{\varepsilon^{min}} (x) | \geq minPts$ or NULL else, 
of each element to its predecessor in the order list and thus, 
a representation of the density necessary to keep two consecutive objects in the same cluster \cite{OPTICS2013}.
If $\varepsilon < RD(y)$, then $y$ is not density reachable from predecessor $x$ and thus, 
one can determine whether $x$ and $y$ are in the same cluster for a given $\varepsilon$ \cite{OPTICS2013}.
According to \citeauthor{OPTICS2013}, the algorithm builds a spanning tree, which enables obtaining the clusters for a given $\varepsilon$ by returning the connected components 
of the spanning tree after omitting all edges with $\varepsilon < RD(y)$ \cite{OPTICS2013}.
The relationship between $\varepsilon$, cluster density and nested density-based clusters is displayed in \autoref{fig:nested_density_cluster}.

% TODO 1999 image of nested clusters to explain eps relation for fixed minPts
\begin{figure}[htp] % htp = hier (h), top (t), oder auf einer eigenen Seite (p).
    \centering
    \includesvg[width=0.5\textwidth]{images/nested_density_cluster.svg}
    \caption{The relationship between $\varepsilon$, cluster density and nested density-based clusters cf. \cite{OPTICS1999}.
    For a constant $minPts$, clusters with higher density such as $C_1$, $C_2$ and $C_3$, i.e. a low $\varepsilon_2$ value, 
    are completely contained in lower density clusters such as $C$ given $\varepsilon_1 > \varepsilon_2$.
    This idea forms the basis of OPTICS of expanding clusters iteratively and thus, 
    enables the detection of clusters for a broad range of neighbourhood radii $0 \le \varepsilon_i \le \varepsilon$.
    }
    \label{fig:nested_density_cluster}
\end{figure}

Hence, this procedure enables the extraction of clusters for arbitrary $0 \le \varepsilon_i \le \varepsilon$ \cite{OPTICS_kMeans_2016, OPTICS1999}.
According to \citeauthor{OPTICS2013}'s work, even though the clsutering algorithm is expensive the extraction needs only linear time.
However, the algorithm is sensitive to the input parameters $minPts$ and $\varepsilon$.
% generating distance

% core distance p





\subsubsection{Variational Bayesian estimation of a Gaussian mixture}\label{subsec:varbayes}

\subsubsection{Annoy}\label{subsec:annoy}


