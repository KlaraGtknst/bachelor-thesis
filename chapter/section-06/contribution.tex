\subsection{Contribution}\label{subsec:contribution}
% mein Beitrag
% literature review, semantic -> visual
An exhaustive literature review was conducted to discover suitable embedding methods for the task of finding similar documents.
Initially, only semantic embedding methods were considered.
However, when the first results were inspected the idea arose to group the documents by visual similarity.
Thus, visual embedding methods were included in this work.

% using existing models
The semantic embeddings were generated by existing models.
All models offer adequate documentation and were thus, mostly comfortable to use.
However, some alterations had to be made to the models to make them suitable for the task.
For instance, a custom preprocessor was implemented for the \ac{tfidf} embedding method.
The preprocessor was a better fit for the task than the default preprocessor.
Moreover, a custom \ac{w2v} model was trained on the dataset.
This \ac{w2v} model is used by \infersent{} reducing the time necessary to encode texts.

% eigenfaces -> eigendocs
The \eigenfaces{} approach has been prevalent since \citeyear{eigenfaces1991}.
In this work, the approach was adapted to the task of finding similar documents and thus, called \eigendocs{}.
The idea of projecting items into a lower dimensional space was kept as well as the preprocessing steps.
However, the preprocessing was extended by placing the document images onto a white canvas.


% clustering
In order to find a suitable clustering algorithm for the task, literature research was conducted.
The research revealed that the \ac{optics} algorithm is a suitable candidate for the task.
In this work, two different preprocessing steps were implemented.
The first was similar to the preprocessing steps used in \cite{OPTICS1999}.
The second one utilized \eigendocs{} to reduce the dimensionality of the data set.
The parameters of the algorithm were chosen in consideration of the reachability plot.


% pca components/ AE architecture evaluation
The data was compressed using \ac{pca} or the encoder of an \ac{ae}.
Both the \ac{pca} and \ac{ae} configurations were experimentally evaluated.
The number of components of the \ac{pca} was chosen in consideration of the cumulative explained variance 
and an adaption of the reconstruction error \ac{rsme}.
The architecture of the \ac{ae} was chosen in consideration of the \ac{rsme} and 
the cosine similarity between the original and the reconstructed vector.


% database local & server
Before the data could be used for the task, it had to be stored in a database.
Firstly, the type of database had to be chosen.
It became clear that a document database would be the best fit for the task 
since it accepts documents of variable shape.
\databaseName{} is a well-known document database that is used by 
well-established companies successfully handling big data.
Secondly, the database had to be configured.
This required the consideration of different field types and similarity measures.
The dense vector seemed to be a native choice and cosine similarity was chosen 
among other reasons due to its popularity as a similarity metric in \ac{vsm}.
Afterwards pipelines to store the data in the database were implemented.
The local database stores 2048 randomly chosen documents from the whole data set.
The server database currently stores around $497504$ incomplete documents.

% evaluation: venn & heatmap, mean/ std
Since the dataset is not labeled, the evaluation of the results is not trivial.
Therefore, multiple evaluation methods were implemented.
The first method is a Venn diagram that depicts the intersection of the query responses of the power set of different methods.
The second method is a heatmap that shows the average portion of shared response documents between different methods.
Lastly, the mean and standard deviation of the portion of shared response documents were calculated 
to further investigate the distribution of the results obtained above.

% topic modeling
To ensure \wordcloud{}s display only valid words the tokens were lemmatized before applying the \wordcloud{} implemented by \citeauthor{wordcloud-dev}. 
The existing functionalities implemented by the library \ac{t2v} were bundled into a class 
which accepts inputs native to the task at hand.
Hence, the functionalities were not extended but rather adapted to the task.  


% chapter about motivation -> goals, techniques used and COMBINED
As initially mentioned, this thesis aims to provide computational means to facilitate the work with large unstructured text data.
In the course of this work, multiple \ac{ml} techniques were examined and evaluated on the task of finding similar documents.
The semantic embedding methods \ac{tfidf}, \ac{d2v}, \infersent{}, \ac{use} and \ac{sbert} were examined.
\eigendocs{} was implemented as a visual embedding method.
It was possible to find a suitable database to store the data.
A pipeline to preprocess, embed and store the data in the database was developed.
The pipeline is scalable to large datasets.
However, models such as \ac{tfidf} yield less meaningful results on certain query documents and 
are prone to performance deterioation with increasing document corpus, since the embeddings' 
dimensionality correlates directly with the vocabulary size.
When the number of terms in the data corpus increases, but the vocabulary size is static, 
it is likely that the vocabulary lacks important words. 

Moreover, a tool was implemented.
The tool provides the possibility to conduct text queries.
A document of interest can be examined in more detail:
The detail component not only contains a \ac{pdf} viewer, 
a \wordcloud{} of the most frequent words in the document, but also an option to examine the query responses for different embeddings.
The file names of the ten most similar documents and a \wordcloud{} of the most frequent words among those documents is displayed.

An experimental evaluation of the embeddings was conducted.
The visual embedding methods were found to be inferior to the semantic embedding methods on certain query documents.
Moreover, the tool was compared to a baseline topic modeling approach called \ac{t2v}.
The library \ac{t2v} implements similar functionalities to the tool developed in this work.
However, the tool is not designed for a productive environment since the focus is on the comparison of different models rather than usability.