\section{Contribution}\label{sec:contribution}

% introduction
In the context of this thesis, several \ac{ml} techniques to derive semantic and visual information 
from unstructured data are explored.

% searchable corpus
In order to find relevant documents in a text corpus, the corpus is made searchable.
This is achieved by constructing a pipeline that preprocesses the documents 
and stores them in a database in an offline fashion.
The local \databaseName{} database stores 2048 randomly chosen documents from the whole dataset, 
whereas the database on the IES server stores around $497504$ incomplete documents.
Owing to \databaseName{}'s implementation, (fuzzy) text queries and \ac{knn} search queries 
can be conducted with minimal latency (cf. \autoref{sec:db}).

% visual method: Eigendocs
Concerning \ref{enum:rq1}, the visual embedding method \eigendocs{} is implemented.
It is an adaption of the prevalent \eigenfaces{} approach to the task of finding similar documents.
The idea of projecting items into a lower dimensional space is kept.
The preprocessing is extended by placing the document images onto a white canvas 
as described in \autoref{subsubsec:eigendocs}.

% semantic methods: tfidf, doc2vec, infersent, use, sbert
The semantic embedding methods \ac{tfidf}, \ac{d2v}, \infersent{}, \ac{use} and \ac{sbert} are explored.
The configurations of the models are altered to reduce their runtime.
Since the dimensionalities of \ac{tfidf} and \infersent{} embeddings are too big 
to be stored in an \databaseName{} database, 
the dimensionality of the embeddings is reduced by the encoder of an \ac{ae} 
(cf. \autoref{subsubsec:impl-autoencoder}).
 
% FE for expert
In the matter of \ref{enum:rq3}, a web interface is implemented, 
which provides the possibility to conduct text queries and
allows to examine a document of interest in more detail (cf. \autoref{sec:ui}):
The detail component not only contains a PDF viewer, 
\wordcloud{}s of the most frequent words in the document or query response, 
but also an option to display the file names of the response documents for different embeddings.
The tool implemented in this work is not designed for a productive environment since 
the focus is on the comparison of different models rather than usability.

% evaluation approaches: venn & heatmap, mean/ std
Since the dataset is not labeled, the evaluation of the results is not trivial.
Therefore, with regard to \ref{enum:rq2} and \ref{enum:rq4} 
multiple evaluation methods are implemented (cf. \autoref{sec:evaluation-models}).
The first method is a Venn diagram that depicts the intersection of the query responses 
of the power set of different embedding models.
The second method is a heatmap that illustrates the average portion of shared response documents 
between different embedding models.
Moreover, the mean and standard deviation of the portion of shared response documents are calculated 
to further investigate the distribution of the results obtained above.
Furthermore, the time consumption of the computation of embeddings for different models is evaluated 
(cf. \autoref{subsec:evaluation-db} \& \autoref{sec:eval-embeddings}).
% baseline: top2vec
Lastly, in \autoref{subsec:top2vec}, the tool is compared to a baseline topic analysis approach called \ac{t2v}.