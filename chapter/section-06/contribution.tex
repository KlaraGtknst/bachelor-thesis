\subsection{Contribution}\label{subsec:contribution}
% mein Beitrag
% literature review, semantic -> visual
An exhaustive literature review was conducted to discover suitable embedding methods for the task of finding similar documents.
Initially, only semantic embedding methods were considered.
However, when the first results were inspected the idea arose to group the documents by visual similarity.
Thus, visual embedding methods were included in this work.

% using existing models
The semantic embeddings are generated by existing models.
All models offer adequate documentation and are thus, mostly comfortable to use.
However, some alterations had to be made to the models to make them suitable for the task.
For instance, a custom preprocessor is implemented for the \ac{tfidf} embedding method.
The preprocessor is a better fit for the task than the default preprocessor.
Moreover, a custom \ac{w2v} model is trained on the dataset.
This \ac{w2v} model is used by \infersent{} reducing the time necessary to encode texts.

% eigenfaces -> eigendocs
The \eigenfaces{} approach has been prevalent since \citeyear{eigenfaces1991}.
In this work, the approach is adapted to the task of finding similar documents and thus, called \eigendocs{}.
The idea of projecting items into a lower dimensional space is kept as well as the preprocessing steps.
However, the preprocessing is extended by placing the document images onto a white canvas.


% clustering
In order to find a suitable clustering algorithm for the task, literature research was conducted.
The research revealed that the \ac{optics} algorithm is a suitable candidate for the task.
In this work, two different preprocessing steps are implemented.
The first is similar to the preprocessing steps used in \cite{OPTICS1999}.
The second one utilizes \eigendocs{} to reduce the dimensionality of the data set.
The parameters of the algorithm are chosen in consideration of the reachability plot.


% pca components/ AE architecture evaluation
The data is compressed using \ac{pca} or the encoder of an \ac{ae}.
Both the \ac{pca} and \ac{ae} configurations are experimentally evaluated.
The number of components of the \ac{pca} is chosen in consideration of the cumulative explained variance 
and an adaption of the reconstruction error \ac{rsme}.
The architecture of the \ac{ae} is chosen in consideration of the \ac{rsme} and 
the cosine similarity between the original and the reconstructed vector.


% database local & server
Before the data could be used for the task, it had to be stored in a database.
Firstly, the type of database had to be chosen.
It became clear that a document database would be the best fit for the task 
since it accepts documents of variable shape.
\databaseName{} is a well-known document database that is used by 
well-established companies successfully handling big data.
Secondly, the database had to be configured.
This required the consideration of different field types and similarity measures.
The dense vector seemed to be a native choice and cosine similarity was chosen 
among other reasons due to its popularity as a similarity metric in \ac{vsm}.
Afterwards pipelines to store the data in the database were implemented.
The local database stores 2048 randomly chosen documents from the whole data set.
The server database currently stores around $497504$ incomplete documents.

% evaluation: venn & heatmap, mean/ std
Since the dataset is not labeled, the evaluation of the results is not trivial.
Therefore, multiple evaluation methods are implemented.
The first method is a Venn diagram that depicts the intersection of the query responses of the power set of different methods.
The second method is a heatmap that shows the average portion of shared response documents between different methods.
Lastly, the mean and standard deviation of the portion of shared response documents are calculated 
to further investigate the distribution of the results obtained above.

% topic analysis
To ensure \wordcloud{}s display only valid words the tokens are lemmatized before applying the \wordcloud{} implemented by \citeauthor{wordcloud-dev}. 
The existing functionalities implemented by the library \ac{t2v} are bundled into a class 
which accepts inputs native to the task at hand.
Hence, the functionalities are not extended but rather adapted to the task.  


% chapter about motivation -> goals, techniques used and COMBINED
As initially mentioned, this thesis aims to provide computational means to facilitate the work with large unstructured text data.
In the course of this work, multiple \ac{ml} techniques are examined and evaluated on the task of finding similar documents.
The semantic embedding methods \ac{tfidf}, \ac{d2v}, \infersent{}, \ac{use} and \ac{sbert} are explored.
\eigendocs{} is implemented as a visual embedding method.
It was possible to find a suitable database to store the data.
A pipeline to preprocess, embed and store the data in the database was developed.
The pipeline is scalable to large datasets.
However, models such as \ac{tfidf} yield less meaningful results on certain query documents and 
are prone to performance deterioration with increasing document corpus, since the embeddings' 
dimensionality correlates directly with the vocabulary size.
When the number of terms in the data corpus increases, but the vocabulary size is static, 
the vocabulary likely lacks important words. 

Moreover, a web interface is implemented.
This tool provides the possibility to conduct text queries.
A document of interest can be examined in more detail:
The detail component not only contains a \ac{pdf} viewer, 
a \wordcloud{} of the most frequent words in the document, but also an option to examine the query responses for different embeddings.
The file names of the ten most similar documents and a \wordcloud{} of the most frequent words among those documents are displayed.

An experimental evaluation of the embeddings was conducted.
The visual embedding methods are considered to be inferior to the semantic embedding methods on certain query documents.
Moreover, the tool was compared to a baseline topic analysis approach called \ac{t2v}.
The library \ac{t2v} implements similar functionalities to the tool developed in this work.
However, the tool is not designed for a productive environment since the focus is on the comparison of different models rather than usability.