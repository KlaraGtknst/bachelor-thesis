@online{wordcloud-dev,
  author = {Andreas Müller},
  title = {word cloud},
  url = {https://github.com/amueller/word_cloud/tree/main},
  date = {2099-09-22},
  note = "[Accessed 05.10.2023]"
}

@report{topic_modeling2015,
    author = {Rubayyi Alghamdi and Khalid Alfalqi},
    title = {A Survey of Topic Modeling in Text Mining},
    journal = {International Journal of Advanced Computer Science and Applications},
    year = {2015},
    volume = {6}
}

% new
@article{evolution_of_topic_modeling2022,
author = {Churchill, Rob and Singh, Lisa},
title = {The Evolution of Topic Modeling},
year = {2022},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {10s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3507900},
doi = {10.1145/3507900},
abstract = {Topic models have been applied to everything from books to newspapers to social media posts in an effort to identify the most prevalent themes of a text corpus. We provide an in-depth analysis of unsupervised topic models from their inception to today. We trace the origins of different types of contemporary topic models, beginning in the 1990s, and we compare their proposed algorithms, as well as their different evaluation approaches. Throughout, we also describe settings in which topic models have worked well and areas where new research is needed, setting the stage for the next generation of topic models.},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {215},
numpages = {35},
keywords = {social media, online topic modeling, Topic modeling, temporal topic modeling}
}

@ARTICLE{topic_modeling2019,
    author={Pooja  Kherwa and Poonam Bansal},
    title={Topic Modeling: A Comprehensive Review},
    journal={EAI Endorsed Transactions on Scalable Information Systems},
    volume={7},
    number={24},
    publisher={EAI},
    journal_a={SIS},
    year={2019},
    month={7},
    keywords={Topic Modeling, Latent Dirichlet Allocation, Latent Semantic Analysis, Inference, Dimension reduction},
    doi={10.4108/eai.13-7-2018.159623}
}

@article{topic_modeling2020,
title = {A review of topic modeling methods},
journal = {Information Systems},
volume = {94},
pages = {101582},
year = {2020},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2020.101582},
url = {https://www.sciencedirect.com/science/article/pii/S0306437920300703},
author = {Ike Vayansky and Sathish A.P. Kumar},
keywords = {Topic modeling, Probabilistic Bayesian networks, Text analysis, Topic correlation, Temporal analysis, Social Media analysis, Inference algorithms},
abstract = {Topic modeling is a popular analytical tool for evaluating data. Numerous methods of topic modeling have been developed which consider many kinds of relationships and restrictions within datasets; however, these methods are not frequently employed. Instead many researchers gravitate to Latent Dirichlet Analysis, which although flexible and adaptive, is not always suited for modeling more complex data relationships. We present different topic modeling approaches capable of dealing with correlation between topics, the changes of topics over time, as well as the ability to handle short texts such as encountered in social media or sparse text data. We also briefly review the algorithms which are used to optimize and infer parameters in topic modeling, which is essential to producing meaningful results regardless of method. We believe this review will encourage more diversity when performing topic modeling and help determine what topic modeling method best suits the user needs.}
}
  
@article{topic_modeling2021, 
  title={Visualizing Topic Models}, 
  year={2021},
  volume={6}, 
  url={https://ojs.aaai.org/index.php/ICWSM/article/view/14321}, 
  DOI={10.1609/icwsm.v6i1.14321}, 
  author={Chaney, Allison and Blei, David}, year={2021}, month={Aug.}, 
  pages={419-422} 
}

@article{dim_reduction2021,
  title = {Conceptual and empirical comparison of dimensionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE)},
  journal = {Computer Science Review},
  volume = {40},
  pages = {100378},
  year = {2021},
  issn = {1574-0137},
  doi = {https://doi.org/10.1016/j.cosrev.2021.100378},
  url = {https://www.sciencedirect.com/science/article/pii/S1574013721000186},
  author = {Farzana Anowar and Samira Sadaoui and Bassant Selim},
}

@article{lda2012,
  title = {TopicTiling: A Text Segmentation Algorithm Based on LDA},
  pages = {37-42},
  year = {2012},
  author = {Martin Riedl and Chris Biemann},
}

@article{text_mining2016,
  title = {A TEXT MINING RESEARCH BASED ON LDA TOPIC MODELLING},
  pages = {201–210},
  year = {2016},
  author = {Zhou Tong and Haiyi Zhang},
}

@INPROCEEDINGS{LDA2016,
  author={Chen, Qiuxing and Yao, Lixiu and Yang, Jie},
  booktitle={2016 International Conference on Audio, Language and Image Processing (ICALIP)}, 
  title={Short text classification based on LDA topic model}, 
  year={2016},
  pages={749-753},}

@INPROCEEDINGS{lda2008,
  author={Wang, Ziqiang and Qian, Xu},
  booktitle={2008 International Conference on Computer Science and Software Engineering}, 
  title={Text Categorization Based on LDA and SVM}, 
  year={2008},
  pages={674-677},
}

@article{bertopic2022,
  title = {BERTopic: Neural topic modeling with a class-based TF-IDF procedure},
  year = {2022},
  author = {Maarten Grootendorst},
}

@article{Top2Vec2020,
  title = {Top2Vec: Distributed Representations of Topics},
  year = {2020},
  author = {Dimo Angelov},
}

